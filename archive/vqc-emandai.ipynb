{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72107c71-453a-4eef-8689-d52b05a012a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## PHOBERT FINE-TUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38b5711a-641c-46f2-8788-692e61d2eb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a992012-357c-43b3-8ad3-c27512b23308",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhosjiu\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/emandai/intent-classifier/runs/s2pyy2lh\" target=\"_blank\">Fine-tuning</a></strong> to <a href=\"https://wandb.ai/emandai/intent-classifier\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/emandai/intent-classifier/runs/s2pyy2lh?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f1fda1beca0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" WANDB SETTINGS \"\"\"\n",
    "\n",
    "wandb.init(project=\"INTENT-CLASSIFIER\",\n",
    "           entity=\"emandai\",\n",
    "           name=\"Fine-tuning\",\n",
    "           save_code=True,\n",
    "           notes=\"Fine-tuning\",\n",
    "           tags=[\"fine-tune\", \"all-layer\", \"[TRAIN] no *other*\", \"[TEST] 100-calls\"],\n",
    "           config={\n",
    "               \"epochs\": 60,\n",
    "               \"lr\": 2e-5,\n",
    "               \"batch_size\": 16,\n",
    "               \"gradient_accumulation_steps\": 2,\n",
    "               \"weight_decay\": 0.01,\n",
    "               \"warmup_ratio\": 0.06,\n",
    "               \"lr_scheduler_type\": \"linear\"\n",
    "           })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "809b9556-8034-4132-8775-d865603fd436",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" WANDB LOGIN \"\"\"\n",
    "\n",
    "wandb.login(relogin=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c0c4923-0732-48d4-beec-0864e97dfa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import os\n",
    "\n",
    "from pyvi import ViTokenizer\n",
    "from vncorenlp import VnCoreNLP\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from datasets import load_metric\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "\n",
    "from config import VQC_DATAPATH, phobert_base_checkpoint\n",
    "import utils\n",
    "from utils import count_trainable_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "442e28f7-4eb4-479d-909d-4bf11fe662e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" LOAD TOKENIZER \"\"\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(phobert_base_checkpoint, use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b554c191-1100-447e-b51d-7c8f69363e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      " 'input_ids': [0, 10122, 29425, 14788, 46, 10578, 59, 8, 1701, 34412, 2],\n",
      " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "\"\"\" TEST Tokenizer \"\"\"\n",
    "\n",
    "text = \"Bánh Xôi Tiêu ông Mẫn rất là ngon.\"\n",
    "tokenized_text = tokenizer(text)\n",
    "pprint(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1923914-f278-4db7-b470-a012808c5f06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> Bánh Xôi Tiêu ông Mẫn rất là ngon. </s>'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" TRY DECODE FROM LIST OF GIVEN TOKENS \"\"\"\n",
    "\n",
    "tokenizer.decode(tokenized_text[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1af5d45-ae5b-4c81-a2cd-d5c26c9be39e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'< s >'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Padding Token \"\"\"\n",
    "\n",
    "tokenizer.decode(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0a47ef8-29e9-485b-8e18-34dbc6a9b637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'< u n k >'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Unknown token \"\"\"\n",
    "\n",
    "tokenizer.decode(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4a57c9c-6e23-4d95-9d7f-f67b7fb8485e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [1, 1, 1, 1, 1, 1, 1],\n",
      " 'input_ids': [0, 765, 1698, 17987, 57, 193, 2],\n",
      " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "\"\"\" TEST Tokenizer \"\"\"\n",
    "\n",
    "text = \"ký rẹt đi em\"\n",
    "tokenized_text = tokenizer(text)\n",
    "pprint(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d64d8c3a-1a5f-437a-abb2-0a6f39d28a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 64001\n",
      "hidden size: 768\n",
      "num attention heads: 12\n",
      "num blocks: 12\n",
      "num labels: 2\n"
     ]
    }
   ],
   "source": [
    "\"\"\" TRY TO LOAD MODEL CONFIGURATION \"\"\"\n",
    "\n",
    "model_config = AutoConfig.from_pretrained(phobert_base_checkpoint)\n",
    "print(f\"vocab size: {model_config.vocab_size}\")\n",
    "print(f\"hidden size: {model_config.hidden_size}\")\n",
    "print(f\"num attention heads: {model_config.num_attention_heads}\")\n",
    "print(f\"num blocks: {model_config.num_hidden_layers}\")\n",
    "print(f\"num labels: {model_config.num_labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe2abe8-9425-40c1-9e51-03930653504d",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d19e42-2bad-463c-b045-14054d053cc3",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b0970c0-4852-48b5-b400-533cedd32283",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" LOAD DATA \"\"\"\n",
    "\n",
    "DATA_PATH = os.path.join(VQC_DATAPATH, \"data_no_other.xlsx\")\n",
    "data_df = pd.read_excel(DATA_PATH)\n",
    "data_df.head()\n",
    "\n",
    "X = data_df[\"Sample\"].tolist()\n",
    "y = data_df[\"Intent\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5060ac3-7b1f-40a2-b8f5-de4853215227",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92554a8d-6c65-47bb-a916-ae1fb69f8951",
   "metadata": {},
   "source": [
    "##### Text Lowering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6188c44f-4dd5-42ea-ac76-0667ed5784dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TEXT LOWERING \"\"\"\n",
    "\n",
    "X = [text.lower() for text in X]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7aa76ee-3162-423e-9c8f-27a1c76846f7",
   "metadata": {},
   "source": [
    "##### Word Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5eaa73fb-b725-441d-92b1-b71b0332c3ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' WORD SEGMENTATION '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" WORD SEGMENTATION \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378426d5-ec01-401b-9d63-a5113d95b4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyvi\n",
    "X = [ViTokenizer.tokenize(text) for text in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67fce080-6e26-4c27-9cbe-dcbf591a6b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDRSegmenter\n",
    "annotator = VnCoreNLP(\"vncorenlp/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size=\"-Xmx500m\")\n",
    "\n",
    "def word_segmenter(f):\n",
    "    _concat = lambda x: \" \".join([token for token in x[0]])\n",
    "    def wrapper(*args, **kwargs):\n",
    "        list_of_tokens = f(*args, **kwargs)\n",
    "        return _concat(list_of_tokens)\n",
    "    return wrapper\n",
    "\n",
    "@word_segmenter\n",
    "def _ws(text):\n",
    "    return annotator.tokenize(text)\n",
    "\n",
    "X = [_ws(text) for text in X]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0825c716-8aa3-4b55-b1dd-ebb22efea580",
   "metadata": {},
   "source": [
    "##### Label Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49b97c8e-3ce1-4841-9e0e-9aa0f5fa9ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Verbose Labels]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "       34, 35, 36, 37, 38, 39, 40, 41])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" LABEL ENCODING (convert label from string to numeric data)\"\"\"\n",
    "\n",
    "lb = LabelEncoder()\n",
    "y = lb.fit_transform(y)\n",
    "print(\"[Verbose Labels]\")\n",
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87a9c1fc-adaa-4d39-81c9-5fb3b36bce69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# samples: 1062\n"
     ]
    }
   ],
   "source": [
    "print(f\"# samples: {len(y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062fba33-6849-4767-b4dd-5e5111952a89",
   "metadata": {},
   "source": [
    "##### (Train, Val, Test) Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20a60fcb-312e-403d-96ae-142aefb87050",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" DATA SPLITING FOR TRAINING, EVALUATING AND TESTING \"\"\"\n",
    "\n",
    "TEST_SIZE = 0.1\n",
    "VAL_SIZE = 0.1\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(X, y, test_size=TEST_SIZE, random_state=42)\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=VAL_SIZE, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9baac8f1-1510-4486-aad3-5933928d5a0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42, 39, 38)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(train_labels)), len(np.unique(val_labels)), len(np.unique(test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bc4518-f667-4f6a-83ba-d914fcb08b8b",
   "metadata": {},
   "source": [
    "##### Text Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5eaaa53-b49d-4295-8a42-34410fc6f297",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TOKENIZE FOR EACH SPLIT \"\"\"\n",
    "\n",
    "train_encodings = tokenizer(train_texts, padding=\"max_length\", truncation=True)\n",
    "val_encodings = tokenizer(val_texts, padding=\"max_length\", truncation=True)\n",
    "test_encodings = tokenizer(test_texts, padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c1991f-bfee-47fa-853c-49d772955869",
   "metadata": {},
   "source": [
    "#### Create PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7bb1d516-e9cc-4517-93e9-8e18bd97ebf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" CREATE DATASET (including `encodings` and `labels`) \"\"\"\n",
    "\n",
    "class VqcDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {}\n",
    "        for key, val in self.encodings.items():\n",
    "            item.update({key: torch.tensor(val[idx])})\n",
    "        item.update({\"label\": torch.tensor(self.labels[idx])})\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    # @classmethod\n",
    "    def get_num_labels(self):\n",
    "        return len(np.unique(self.labels))\n",
    "\n",
    "train_dataset = VqcDataset(train_encodings, train_labels)\n",
    "val_dataset = VqcDataset(val_encodings, val_labels)\n",
    "test_dataset = VqcDataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88d5a5e-9fe8-4c8d-824f-7afcf34597f5",
   "metadata": {},
   "source": [
    "### Load Pre-trained PhoBERT-base checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ac00857-b9a1-43ca-bdf0-064b210026a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./results/TAPT/checkpoint-2500 were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./results/TAPT/checkpoint-2500 and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "👇 MODEL CONFIGURATIONS 👇\n",
      "\n",
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"./results/TAPT/checkpoint-2500\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 258,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"PhobertTokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" LOAD PRE-TRAINED PhoBERT MODEL \"\"\"\n",
    "\n",
    "# We can use RobertaForSequenceClassification as an alternative\n",
    "num_labels = train_dataset.get_num_labels()\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"./results/TAPT/checkpoint-2500\",\n",
    "                                                           num_labels=num_labels)\n",
    "\n",
    "# Re-check model configurations\n",
    "print(f\"👇 MODEL CONFIGURATIONS 👇\\n\")\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cd3230b5-636d-4015-ae34-e89a8744626a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./results/pretraining/checkpoint-100000/ were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./results/pretraining/checkpoint-100000/ and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" LOAD PRE-TRAINED PhoBERT MODEL \"\"\"\n",
    "\n",
    "# We can use RobertaForSequenceClassification as an alternative\n",
    "num_labels = train_dataset.get_num_labels()\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"./results/pretraining/checkpoint-100000/\",\n",
    "                                                           num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c89f228-4373-440a-b373-38d6f853db58",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Define Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7ab03140-5ca7-4872-afa2-dec0c7f7cc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Metric for model evaluation\n",
    "# Reference: https://huggingface.co/transformers/training.html#fine-tuning-in-pytorch-with-the-trainer-api\n",
    "metric = load_metric(\"f1\")\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    return metric.compute(predictions=predictions,\n",
    "                          references=labels,\n",
    "                          average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9287c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"INTENT-CLASSIFIER\",\n",
    "           entity=\"emandai\",\n",
    "           name=\"Fine-tuning\",\n",
    "           save_code=True,\n",
    "           notes=\"Fine-tuning\",\n",
    "           tags=[\"fine-tune\", \"all-layer\", \"[TRAIN] no *other*\", \"[TEST] 100-calls\"],\n",
    "           config={\n",
    "               \"epochs\": 60,\n",
    "               \"lr\": 2e-5,\n",
    "               \"batch_size\": 16,\n",
    "               \"gradient_accumulation_steps\": 2,\n",
    "               \"weight_decay\": 0.01,\n",
    "               \"warmup_ratio\": 0.06,\n",
    "               \"lr_scheduler_type\": \"linear\"\n",
    "           })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec79ed3a-311c-4283-bf7e-14a36829044d",
   "metadata": {},
   "source": [
    "### Training Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "797dca31-c6e6-470b-9379-3b53623c4602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer Argument\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=wandb.config.epochs,\n",
    "    learning_rate=wandb.config.lr,\n",
    "    per_device_train_batch_size=wandb.config.batch_size,\n",
    "    per_device_eval_batch_size=wandb.config.batch_size,\n",
    "    gradient_accumulation_steps=wandb.config.gradient_accumulation_steps,\n",
    "    weight_decay=wandb.config.weight_decay,\n",
    "    warmup_ratio=wandb.config.warmup_ratio,\n",
    "    lr_scheduler_type=wandb.config.lr_scheduler_type,\n",
    "    logging_steps=10,\n",
    "    eval_steps=100,\n",
    "    save_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\",\n",
    "    save_total_limit=10,\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"wandb\",\n",
    ")\n",
    "\n",
    "# Fine-tuning using Trainer API from Huggingface\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(5)] # Early Stopping\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e27608-5077-4c12-addb-da20056ee2b2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Fine-tuning All Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d78542c1-67fd-4805-a18e-f7e29192b3dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 859\n",
      "  Num Epochs = 60\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 1620\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of trainable parameters: 154714410\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1620' max='1620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1620/1620 09:38, Epoch 60/60]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.359600</td>\n",
       "      <td>3.296652</td>\n",
       "      <td>0.246738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.384200</td>\n",
       "      <td>2.359263</td>\n",
       "      <td>0.476249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.598000</td>\n",
       "      <td>1.673203</td>\n",
       "      <td>0.649381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.037600</td>\n",
       "      <td>1.249442</td>\n",
       "      <td>0.691361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.599500</td>\n",
       "      <td>1.019155</td>\n",
       "      <td>0.742193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.381000</td>\n",
       "      <td>0.862205</td>\n",
       "      <td>0.731624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.241800</td>\n",
       "      <td>0.809447</td>\n",
       "      <td>0.725590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.155200</td>\n",
       "      <td>0.807439</td>\n",
       "      <td>0.725782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.118400</td>\n",
       "      <td>0.806648</td>\n",
       "      <td>0.725782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.102600</td>\n",
       "      <td>0.808311</td>\n",
       "      <td>0.714750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.087200</td>\n",
       "      <td>0.809232</td>\n",
       "      <td>0.714750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.072800</td>\n",
       "      <td>0.804379</td>\n",
       "      <td>0.725782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.073300</td>\n",
       "      <td>0.821179</td>\n",
       "      <td>0.725782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.062400</td>\n",
       "      <td>0.816567</td>\n",
       "      <td>0.725782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.061000</td>\n",
       "      <td>0.822738</td>\n",
       "      <td>0.725782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.064500</td>\n",
       "      <td>0.824428</td>\n",
       "      <td>0.725782</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 96\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-100\n",
      "Configuration saved in ./results/checkpoint-100/config.json\n",
      "Model weights saved in ./results/checkpoint-100/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-700] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 96\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-200\n",
      "Configuration saved in ./results/checkpoint-200/config.json\n",
      "Model weights saved in ./results/checkpoint-200/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-800] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 96\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-300\n",
      "Configuration saved in ./results/checkpoint-300/config.json\n",
      "Model weights saved in ./results/checkpoint-300/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-900] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 96\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-400\n",
      "Configuration saved in ./results/checkpoint-400/config.json\n",
      "Model weights saved in ./results/checkpoint-400/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-1000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 96\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-500\n",
      "Configuration saved in ./results/checkpoint-500/config.json\n",
      "Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-1100] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 96\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-600\n",
      "Configuration saved in ./results/checkpoint-600/config.json\n",
      "Model weights saved in ./results/checkpoint-600/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-1200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 96\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-700\n",
      "Configuration saved in ./results/checkpoint-700/config.json\n",
      "Model weights saved in ./results/checkpoint-700/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-1300] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 96\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-800\n",
      "Configuration saved in ./results/checkpoint-800/config.json\n",
      "Model weights saved in ./results/checkpoint-800/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-1400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 96\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-900\n",
      "Configuration saved in ./results/checkpoint-900/config.json\n",
      "Model weights saved in ./results/checkpoint-900/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-1500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 96\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-1000\n",
      "Configuration saved in ./results/checkpoint-1000/config.json\n",
      "Model weights saved in ./results/checkpoint-1000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-1600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 96\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-1100\n",
      "Configuration saved in ./results/checkpoint-1100/config.json\n",
      "Model weights saved in ./results/checkpoint-1100/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-100] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 96\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-1200\n",
      "Configuration saved in ./results/checkpoint-1200/config.json\n",
      "Model weights saved in ./results/checkpoint-1200/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 96\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-1300\n",
      "Configuration saved in ./results/checkpoint-1300/config.json\n",
      "Model weights saved in ./results/checkpoint-1300/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-300] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 96\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-1400\n",
      "Configuration saved in ./results/checkpoint-1400/config.json\n",
      "Model weights saved in ./results/checkpoint-1400/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 96\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-1500\n",
      "Configuration saved in ./results/checkpoint-1500/config.json\n",
      "Model weights saved in ./results/checkpoint-1500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 96\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-1600\n",
      "Configuration saved in ./results/checkpoint-1600/config.json\n",
      "Model weights saved in ./results/checkpoint-1600/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-600] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results/checkpoint-1200 (score: 0.8043792843818665).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish, PID 1479... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 4.82MB of 4.82MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">Fine-tuning</strong>: <a href=\"https://wandb.ai/emandai/intent-classifier/runs/s2pyy2lh\" target=\"_blank\">https://wandb.ai/emandai/intent-classifier/runs/s2pyy2lh</a><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"total number of trainable parameters: {count_trainable_params(model)}\")\n",
    "\n",
    "# Fine-tuning\n",
    "trainer.train()\n",
    "\n",
    "# Finish wandb\n",
    "wandb.finish(quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c224f6f-9d70-49e3-80a2-0a828e2a156e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Fine-tuning Last Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054a5519-4598-4067-94ea-71989d9ed0f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For embedding layers\n",
    "for name, params in model.roberta.embeddings.named_parameters():\n",
    "    params.requires_grad = False\n",
    "\n",
    "# For encoder layers\n",
    "for name, params in model.roberta.encoder.named_parameters():\n",
    "    params.requires_grad = False\n",
    "\n",
    "    if \"layer.11\" in name:\n",
    "        params.requires_grad = True\n",
    "\n",
    "print(f\"total number of trainable parameters: {count_trainable_params(model)}\")\n",
    "        \n",
    "# Fine-tuning\n",
    "trainer.train()\n",
    "\n",
    "# Finish wandb\n",
    "wandb.finish(quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38a5339-c012-4b9a-8e30-d3215ee82507",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Fine-tune from 11th Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c979a1d7-2792-44da-8323-e77cf0a117cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For embedding layers\n",
    "for name, params in model.roberta.embeddings.named_parameters():\n",
    "    params.requires_grad = False\n",
    "\n",
    "# For encoder layers\n",
    "for name, params in model.roberta.encoder.named_parameters():\n",
    "    params.requires_grad = False\n",
    "\n",
    "    if \"layer.10\" in name:\n",
    "        params.requires_grad = True\n",
    "        continue\n",
    "    \n",
    "    if \"layer.11\" in name:\n",
    "        params.requires_grad = True\n",
    "\n",
    "print(f\"total number of trainable parameters: {count_trainable_params(model)}\")\n",
    "        \n",
    "# Fine-tuning\n",
    "trainer.train()\n",
    "\n",
    "# Finish wandb\n",
    "wandb.finish(quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18d4e90-1299-4a9c-816e-9f59eeeea048",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Fine-tune from 10th Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc68c2d-0c0f-4ae5-8820-b4d6646da1e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For embedding layers\n",
    "for name, params in model.roberta.embeddings.named_parameters():\n",
    "    params.requires_grad = False\n",
    "\n",
    "# For encoder layers\n",
    "for name, params in model.roberta.encoder.named_parameters():\n",
    "    params.requires_grad = False\n",
    "    \n",
    "    if \"layer.9\" in name:\n",
    "        params.requires_grad = True\n",
    "        continue\n",
    "    \n",
    "    if \"layer.10\" in name:\n",
    "        params.requires_grad = True\n",
    "        continue\n",
    "    \n",
    "    if \"layer.11\" in name:\n",
    "        params.requires_grad = True\n",
    "\n",
    "print(f\"total number of trainable parameters: {sum([params.numel() for params in model.parameters() if params.requires_grad])}\")\n",
    "        \n",
    "# Fine-tuning\n",
    "trainer.train()\n",
    "\n",
    "# Finish wandb\n",
    "wandb.finish(quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800575d5-8b19-4579-88ab-28ff2b585c03",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Fine-tune from 9th Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "32a07b95-83ca-40fe-81ba-ce24fe6a07bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 859\n",
      "  Num Epochs = 60\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 1620\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of trainable parameters: 28974378\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1620' max='1620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1620/1620 05:09, Epoch 60/60]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.390400</td>\n",
       "      <td>3.288343</td>\n",
       "      <td>0.277443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.372000</td>\n",
       "      <td>2.308057</td>\n",
       "      <td>0.461140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.705400</td>\n",
       "      <td>1.707848</td>\n",
       "      <td>0.601507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.251700</td>\n",
       "      <td>1.327790</td>\n",
       "      <td>0.695901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.844300</td>\n",
       "      <td>1.087066</td>\n",
       "      <td>0.735015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.591400</td>\n",
       "      <td>0.915444</td>\n",
       "      <td>0.750985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.468100</td>\n",
       "      <td>0.815914</td>\n",
       "      <td>0.772123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.319700</td>\n",
       "      <td>0.750850</td>\n",
       "      <td>0.782963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.248800</td>\n",
       "      <td>0.711245</td>\n",
       "      <td>0.782963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.192300</td>\n",
       "      <td>0.693392</td>\n",
       "      <td>0.773872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.172700</td>\n",
       "      <td>0.676533</td>\n",
       "      <td>0.768065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.130900</td>\n",
       "      <td>0.670143</td>\n",
       "      <td>0.777156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.126100</td>\n",
       "      <td>0.663134</td>\n",
       "      <td>0.768065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.108100</td>\n",
       "      <td>0.659656</td>\n",
       "      <td>0.797267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.107600</td>\n",
       "      <td>0.665430</td>\n",
       "      <td>0.797267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.109800</td>\n",
       "      <td>0.667298</td>\n",
       "      <td>0.797267</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 96\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-100\n",
      "Configuration saved in ./results/checkpoint-100/config.json\n",
      "Model weights saved in ./results/checkpoint-100/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-700] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 96\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-200\n",
      "Configuration saved in ./results/checkpoint-200/config.json\n",
      "Model weights saved in ./results/checkpoint-200/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-800] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 96\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-300\n",
      "Configuration saved in ./results/checkpoint-300/config.json\n",
      "Model weights saved in ./results/checkpoint-300/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-900] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 96\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-400\n",
      "Configuration saved in ./results/checkpoint-400/config.json\n",
      "Model weights saved in ./results/checkpoint-400/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-1000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 96\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-500\n",
      "Configuration saved in ./results/checkpoint-500/config.json\n",
      "Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-1100] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 96\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-600\n",
      "Configuration saved in ./results/checkpoint-600/config.json\n",
      "Model weights saved in ./results/checkpoint-600/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-1200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 96\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-700\n",
      "Configuration saved in ./results/checkpoint-700/config.json\n",
      "Model weights saved in ./results/checkpoint-700/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-1300] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 96\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-800\n",
      "Configuration saved in ./results/checkpoint-800/config.json\n",
      "Model weights saved in ./results/checkpoint-800/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-1400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 96\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-900\n",
      "Configuration saved in ./results/checkpoint-900/config.json\n",
      "Model weights saved in ./results/checkpoint-900/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-1500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 96\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-1000\n",
      "Configuration saved in ./results/checkpoint-1000/config.json\n",
      "Model weights saved in ./results/checkpoint-1000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-1600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 96\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-1100\n",
      "Configuration saved in ./results/checkpoint-1100/config.json\n",
      "Model weights saved in ./results/checkpoint-1100/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-100] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 96\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-1200\n",
      "Configuration saved in ./results/checkpoint-1200/config.json\n",
      "Model weights saved in ./results/checkpoint-1200/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 96\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-1300\n",
      "Configuration saved in ./results/checkpoint-1300/config.json\n",
      "Model weights saved in ./results/checkpoint-1300/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-300] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 96\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-1400\n",
      "Configuration saved in ./results/checkpoint-1400/config.json\n",
      "Model weights saved in ./results/checkpoint-1400/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 96\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-1500\n",
      "Configuration saved in ./results/checkpoint-1500/config.json\n",
      "Model weights saved in ./results/checkpoint-1500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 96\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-1600\n",
      "Configuration saved in ./results/checkpoint-1600/config.json\n",
      "Model weights saved in ./results/checkpoint-1600/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-600] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results/checkpoint-1400 (score: 0.6596562266349792).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 96\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish, PID 4103763... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 4.96MB of 4.96MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">Fine-tuning</strong>: <a href=\"https://wandb.ai/emandai/intent-classifier/runs/380g906f\" target=\"_blank\">https://wandb.ai/emandai/intent-classifier/runs/380g906f</a><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For embedding layers\n",
    "for name, params in model.roberta.embeddings.named_parameters():\n",
    "    params.requires_grad = False\n",
    "\n",
    "# For encoder layers\n",
    "for name, params in model.roberta.encoder.named_parameters():\n",
    "    params.requires_grad = False\n",
    "\n",
    "    if \"layer.8\" in name:\n",
    "        params.requires_grad = True\n",
    "        continue\n",
    "    \n",
    "    if \"layer.9\" in name:\n",
    "        params.requires_grad = True\n",
    "        continue\n",
    "    \n",
    "    if \"layer.10\" in name:\n",
    "        params.requires_grad = True\n",
    "        continue\n",
    "    \n",
    "    if \"layer.11\" in name:\n",
    "        params.requires_grad = True\n",
    "\n",
    "print(f\"total number of trainable parameters: {count_trainable_params(model)}\")\n",
    "        \n",
    "# Fine-tuning\n",
    "trainer.train()\n",
    "\n",
    "# evaluate\n",
    "trainer.evaluate()\n",
    "\n",
    "# Finish wandb\n",
    "wandb.finish(quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4132f2e9-757e-44e3-811b-6c392c566414",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Fine-tuning Classification Head Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb77749e-4195-4529-8a05-0e4e2f966405",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for name, params in model.named_parameters():\n",
    "    if \"classifier\" not in name:\n",
    "        params.requires_grad = False\n",
    "\n",
    "print(f\"total number of trainable parameters: {count_trainable_params(model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94139883-47a3-4006-a95c-ca6608779ebe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fine-tuning\n",
    "trainer.train()\n",
    "\n",
    "# Finish wandb\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ffdd36-d1ce-4274-818c-2dcb578ae8c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ebafd4a9-15c5-4172-aade-6ac88f4d2a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" DEFINE EVALUATE FUNCTION \"\"\"\n",
    "\n",
    "def evaluate(texts, labels, metric=\"f1\"):\n",
    "    preds = []\n",
    "    for text in texts:\n",
    "        input_ids = torch.tensor([tokenizer.encode(text)]).to(device=\"cuda:0\")\n",
    "        logits = model(input_ids).logits\n",
    "        prob = torch.softmax(logits, dim=1)\n",
    "        max_idx = torch.argmax(prob).item()\n",
    "        preds.append(max_idx)\n",
    "\n",
    "    if metric == \"f1\":\n",
    "        f1 = f1_score(labels, preds, average=\"macro\")\n",
    "        return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "424d3ac6-881e-4eea-9647-631850fcddd2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-macro: 0.7495765929976458\n"
     ]
    }
   ],
   "source": [
    "\"\"\" EVALUATE ON TEST SPLIT \"\"\"\n",
    "\n",
    "model.eval()\n",
    "f1 = evaluate(test_texts, test_labels, metric=\"f1\")\n",
    "print(f\"F1-macro: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "149d6b23-c6fb-45e1-a134-c9ebc3a0169c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" LOAD REAL TEST DATA \"\"\"\n",
    "\n",
    "X_test, y_test = utils.get_test(target=\"single\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f8607119-63fd-447f-ae2b-f0bb9a029dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" SEGMENT REAL TEST DATA \"\"\"\n",
    "\n",
    "X_test = [_ws(text) for text in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cb432b6e-f761-46f4-ac39-7c58a63d79fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" LABEL ENCODING FOR REAL TEST DATA \"\"\"\n",
    "\n",
    "y_test = lb.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e1eb4c8c-7aba-4c4b-a299-145132ab1251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-macro: 0.4225098806151392\n"
     ]
    }
   ],
   "source": [
    "\"\"\" EVALUATE ON REAL TEST SET \"\"\"\n",
    "\n",
    "model.eval()\n",
    "f1 = evaluate(X_test, y_test, metric=\"f1\")\n",
    "print(f\"F1-macro: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25bcb83-ab27-4d6f-b0ce-f1d4d330d64e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" CONFUSION MATRIX [PhoBERT Fine-Tuning] \"\"\"\n",
    "\n",
    "cm = confusion_matrix(y_test, preds)\n",
    "\n",
    "print(len(np.unique(preds)))\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bf070d-6764-4750-93ca-88ee3bae8e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" INFERENCE TESTING \"\"\"\n",
    "\n",
    "text = \"ừ em đóng rất là tốt em đóng rất là tốt và có uy_tín của công_ty chị cho_nên công_ty chị đợt này mới ưu_đãi lại cho em một ờ cái khoản vay bằng tiền_mặt\"\n",
    "\n",
    "# encode = tokenize + numericalize\n",
    "input_ids = torch.tensor([tokenizer.encode(text)]).to(device=\"cuda:0\")\n",
    "\n",
    "# forward\n",
    "logit = model(input_ids).logits\n",
    "print(\"Output Logits:\\n\")\n",
    "print(logit, end=\"\\n\\n\")\n",
    "\n",
    "print(\"Output Labels:\")\n",
    "prob = torch.softmax(logit, dim=1)\n",
    "max_idx = torch.argmax(prob).item()\n",
    "lb.inverse_transform(np.array([max_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1e63fd-0062-45e3-a1f7-98586d457aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[1000], lb.inverse_transform([y_test[1000]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704e6025-7837-4d3c-bead-d978c5cf314c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## TASK ADAPTIVE PRE-TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cfa24d67-1df9-4534-a904-34c62bddb9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import wandb\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import datasets\n",
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "from vncorenlp import VnCoreNLP\n",
    "from pyvi import ViTokenizer\n",
    "from joblib import Parallel, delayed\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForMaskedLM,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    pipeline,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "\n",
    "from config import (\n",
    "    VQC_UNLABELED_DATAPATH,\n",
    "    VQC_DATAPATH,\n",
    "    phobert_base_checkpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2666c086-8fa3-49f7-aa6f-3933e0bf0a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "transformers.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "37b6735e-c5ff-4125-be03-f846dbca6ec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/emandai/intent-classifier/runs/3pblif3k\" target=\"_blank\">Task Adaptive Pretraining</a></strong> to <a href=\"https://wandb.ai/emandai/intent-classifier\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/emandai/intent-classifier/runs/3pblif3k?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fa94162db50>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" WANDB SETTINGS \"\"\"\n",
    "\n",
    "wandb.init(project=\"INTENT-CLASSIFIER\",\n",
    "           entity=\"emandai\",\n",
    "           name=\"Task Adaptive Pretraining\",\n",
    "           save_code=True,\n",
    "           tags=[\"pretraining\", \"TAPT\"],\n",
    "           config={\n",
    "               \"epochs\": 100,\n",
    "               \"lr\": 2e-5,\n",
    "               \"batch_size\": 8,\n",
    "               \"gradient_accumulation_steps\": 2,\n",
    "               \"weight_decay\": 0.01,\n",
    "               \"warmup_ratio\": 0.06,\n",
    "               \"lr_scheduler_type\": \"linear\"\n",
    "           })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7b728716-16ca-4d41-b6cb-eedb771bd703",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/vinai/phobert-base/resolve/main/config.json from cache at /home/kiethoang/.cache/huggingface/transformers/a596f267f08b7158c7ab6300b1bf98eb6e1b05e6bcb0d7c18a8070364ee3011b.bbe27b2cac909b2279c83792c2d2b6f159f0a95f5d1c1eb66451da1c89a53609\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 258,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"PhobertTokenizer\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/vinai/phobert-base/resolve/main/pytorch_model.bin from cache at /home/kiethoang/.cache/huggingface/transformers/8363542cfd9e2bad1a9a618e87ea1153d84819a3ae581cff0816a2c1f610f433.42a5e558f15db4cc3af338445707272b8f7545df78efdc125d3fd51025b22d85\n",
      "All model checkpoint weights were used when initializing RobertaForMaskedLM.\n",
      "\n",
      "All the weights of RobertaForMaskedLM were initialized from the model checkpoint at vinai/phobert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" LOAD PHOBERT CHECKPOINT \"\"\"\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(phobert_base_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3461f387-bd63-4fc8-9659-cadd4619fe0e",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "14fcd48c-3e50-4573-965b-ada8a120a35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\" LOAD TRANSCRIBED TEXTS \"\"\"\n",
    "\n",
    "# path = os.path.join(VQC_UNLABELED_DATAPATH, \"transcribed_texts.txt\")\n",
    "# lines_df = pd.read_csv(path, delimiter=\"\\n\", names=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3503c709-88d6-4393-8de0-8cd7ea962bf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anh là Ân đây</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chị là hiếu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>em tên là hà</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>em hà đây</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>em là hương</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Sample\n",
       "0  anh là Ân đây\n",
       "1    chị là hiếu\n",
       "2   em tên là hà\n",
       "3      em hà đây\n",
       "4    em là hương"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_PATH = os.path.join(VQC_DATAPATH, \"pretrain_data_medium.xlsx\")\n",
    "\n",
    "lines_df = pd.read_excel(DATA_PATH, usecols=[\"Sample\"])\n",
    "lines_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494bb526-f694-43d2-b939-ccce8780dc95",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcaa2e09-e9ed-49d7-8cdf-1e0680ea79b2",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Remove Duplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7188aa14-99cd-43fc-93e4-084ca15ee8b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anh là Ân đây</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chị là hiếu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>em tên là hà</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>em hà đây</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>em là hương</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2584</th>\n",
       "      <td>đợt này bên em có mở ra một cái gói vay vốn ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2585</th>\n",
       "      <td>mà không biết là chị cân nhắc được khoản nào k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2586</th>\n",
       "      <td>thì bên em hỗ trợ cho mình nhận về thì số khoả...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2587</th>\n",
       "      <td>ủa giờ như dịch bệnh này mà lỡ may không có ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2588</th>\n",
       "      <td>ủa mà mà mà bây giờ tui đang ở trong khu cách ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2589 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Sample\n",
       "0                                         anh là Ân đây\n",
       "1                                           chị là hiếu\n",
       "2                                          em tên là hà\n",
       "3                                             em hà đây\n",
       "4                                           em là hương\n",
       "...                                                 ...\n",
       "2584  đợt này bên em có mở ra một cái gói vay vốn ti...\n",
       "2585  mà không biết là chị cân nhắc được khoản nào k...\n",
       "2586  thì bên em hỗ trợ cho mình nhận về thì số khoả...\n",
       "2587  ủa giờ như dịch bệnh này mà lỡ may không có ti...\n",
       "2588  ủa mà mà mà bây giờ tui đang ở trong khu cách ...\n",
       "\n",
       "[2589 rows x 1 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" REMOVE DUPLICATE \"\"\"\n",
    "\n",
    "lines_df.drop_duplicates(ignore_index=True, inplace=True)\n",
    "lines_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072e6d5e-c899-431d-9a13-bcd48f1ee6aa",
   "metadata": {},
   "source": [
    "#### Text Lowering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "619c97eb-16f7-43d7-a73a-1b1173c8612a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TEXT LOWERING \"\"\"\n",
    "\n",
    "for index, row in lines_df.iterrows():\n",
    "    lines_df.at[index, \"Sample\"] = row[\"Sample\"].lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9c182f-36a4-442d-8fb4-16a53885e9b3",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Word Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea72f4cb-693c-4042-b71c-1c58cda665b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pyvi\n",
    "ws_texts = Parallel(n_jobs=8)(delayed(ViTokenizer.tokenize)(row[\"text\"]) for _, row in lines_df.iterrows())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a44dc084-e2fe-4613-9d48-11756c28e8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDRSegmenter\n",
    "annotator = VnCoreNLP(\"vncorenlp/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size=\"-Xmx500m\")\n",
    "\n",
    "def word_segmenter(f):\n",
    "    _concat = lambda x: \" \".join([token for token in x[0]])\n",
    "    def wrapper(*args, **kwargs):\n",
    "        list_of_tokens = f(*args, **kwargs)\n",
    "        return _concat(list_of_tokens)\n",
    "    return wrapper\n",
    "\n",
    "@word_segmenter\n",
    "def _ws(text):\n",
    "    return annotator.tokenize(text)\n",
    "\n",
    "\n",
    "ws_texts = [_ws(row[\"Sample\"]) for _, row in lines_df.iterrows()]\n",
    "for index, row in lines_df.iterrows():\n",
    "    lines_df.at[index, \"Sample\"] = _ws(row[\"Sample\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9199adaf-11e7-45d6-a835-dfd5629a5727",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e3e99e75-b289-46ed-b44a-e8b0b34f0e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" INITSTANTIATE HF DATASET FROM PANDAS DATAFRAME \"\"\"\n",
    "dataset = Dataset.from_pandas(df=lines_df)\n",
    "\n",
    "\"\"\" TRAIN TEST SPLIT \"\"\"\n",
    "TEST_SIZE = 0.1\n",
    "dataset = dataset.train_test_split(test_size=TEST_SIZE)\n",
    "dataset[\"validation\"] = dataset.pop(\"test\") # For name consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109c834d-2056-4362-b683-86a4d0e132fb",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Text Tokenization (removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c70fcf6a-5dda-4ecd-8e78-89f799950fe3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/vinai/phobert-base/resolve/main/config.json from cache at /home/kiethoang/.cache/huggingface/transformers/a596f267f08b7158c7ab6300b1bf98eb6e1b05e6bcb0d7c18a8070364ee3011b.bbe27b2cac909b2279c83792c2d2b6f159f0a95f5d1c1eb66451da1c89a53609\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 258,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"PhobertTokenizer\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/vinai/phobert-base/resolve/main/vocab.txt from cache at /home/kiethoang/.cache/huggingface/transformers/970c6224b2713c8b52a7bcfc4d5a951c9bb88302e4523388b50f28284e87ac44.26ba0c8945e559c68d0bc35d24fea16f5463a49fe8f134e0c32261d590b577fa\n",
      "loading file https://huggingface.co/vinai/phobert-base/resolve/main/bpe.codes from cache at /home/kiethoang/.cache/huggingface/transformers/f3a66ae0a78d1a53b3eb99e31837d0d8e2f684a2dcc1f52f75fd36873e3d79de.301ac8958de708ddcea8500d9acbe6261dba391d249c98dcda1e49dbbff870dd\n",
      "loading file https://huggingface.co/vinai/phobert-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/vinai/phobert-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/vinai/phobert-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading file https://huggingface.co/vinai/phobert-base/resolve/main/tokenizer.json from cache at None\n",
      "loading configuration file https://huggingface.co/vinai/phobert-base/resolve/main/config.json from cache at /home/kiethoang/.cache/huggingface/transformers/a596f267f08b7158c7ab6300b1bf98eb6e1b05e6bcb0d7c18a8070364ee3011b.bbe27b2cac909b2279c83792c2d2b6f159f0a95f5d1c1eb66451da1c89a53609\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 258,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"PhobertTokenizer\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n",
      "Adding <mask> to the vocabulary\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(phobert_base_checkpoint, use_fast=True)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "10743847-6be9-41f2-b451-e827cd872082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\" ADD NEW TOKENS TO VOCAB \"\"\"\n",
    "\n",
    "# # build vocab from the train set\n",
    "# def yield_tokens_from_hf_dataset(dataset):\n",
    "#     for text in dataset[\"text\"]:\n",
    "#         yield text.split()\n",
    "# vocab = build_vocab_from_iterator(iterator=yield_tokens_from_hf_dataset(dataset[\"train\"]))\n",
    "\n",
    "# # add new tokens to the tokenizer\n",
    "# num_added_tokens = tokenizer.add_tokens(list(vocab.stoi)[2:]) # remove <pad> and <unk> token\n",
    "\n",
    "# print(f\"[BEFORE] vocab size: {tokenizer.vocab_size}\")\n",
    "# print(f\"[AFTER] vocab size: {len(tokenizer)} (+{len(tokenizer) - tokenizer.vocab_size})\")\n",
    "\n",
    "# # Update embeddings matrix size\n",
    "# # model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b343c5d-36db-456a-b376-82377c7211e4",
   "metadata": {},
   "source": [
    "### Training settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "939edad1-4de9-4a13-b0d8-011fbb77bf1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "\"\"\" TRAINING SETTINGS \"\"\"\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    num_train_epochs=wandb.config.epochs,\n",
    "    learning_rate=wandb.config.lr,\n",
    "    lr_scheduler_type=wandb.config.lr_scheduler_type,\n",
    "    weight_decay=wandb.config.weight_decay,\n",
    "    warmup_ratio=wandb.config.warmup_ratio,\n",
    "    per_device_train_batch_size=wandb.config.batch_size,\n",
    "    per_device_eval_batch_size=wandb.config.batch_size,\n",
    "    gradient_accumulation_steps=wandb.config.gradient_accumulation_steps,\n",
    "    logging_steps=10,\n",
    "    logging_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_total_limit=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\",\n",
    "    logging_dir=\"./logs/TAPT\",\n",
    "    output_dir=\"./results/TAPT\",\n",
    "    report_to=\"wandb\"\n",
    ")\n",
    "\n",
    "# Random mask\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm_probability=0.15\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[EarlyStoppingCallback(5)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a6f5ea-46bc-4dc6-8f2b-c85328fa5dd8",
   "metadata": {},
   "source": [
    "### Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0b72e2fa-6b4d-40f2-8ed6-bfd190a80328",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 2330\n",
      "  Num Epochs = 100\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 14600\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3000' max='14600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 3000/14600 15:08 < 58:35, 3.30 it/s, Epoch 20/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>4.416700</td>\n",
       "      <td>4.510520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.942100</td>\n",
       "      <td>3.764014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.941400</td>\n",
       "      <td>3.510015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.155700</td>\n",
       "      <td>3.038373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.808900</td>\n",
       "      <td>2.948938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.861100</td>\n",
       "      <td>2.961706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.770300</td>\n",
       "      <td>2.701191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.451000</td>\n",
       "      <td>2.583792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.524000</td>\n",
       "      <td>2.395486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.741400</td>\n",
       "      <td>2.558349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>2.564300</td>\n",
       "      <td>2.545668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.993200</td>\n",
       "      <td>2.402087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>2.338800</td>\n",
       "      <td>2.340636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.274600</td>\n",
       "      <td>2.393654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.043600</td>\n",
       "      <td>2.426581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.271800</td>\n",
       "      <td>2.335215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>2.266500</td>\n",
       "      <td>2.396554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>2.027600</td>\n",
       "      <td>2.370804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>1.800100</td>\n",
       "      <td>2.314512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.820600</td>\n",
       "      <td>2.294625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>1.910500</td>\n",
       "      <td>2.371946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>1.856300</td>\n",
       "      <td>2.167325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>1.939800</td>\n",
       "      <td>2.182088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>1.707300</td>\n",
       "      <td>2.302521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.636600</td>\n",
       "      <td>2.090914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>1.774400</td>\n",
       "      <td>2.144854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>1.565500</td>\n",
       "      <td>2.353202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>2.139600</td>\n",
       "      <td>2.402102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>1.787500</td>\n",
       "      <td>2.258165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.935400</td>\n",
       "      <td>2.241846</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/TAPT/checkpoint-100\n",
      "Configuration saved in ./results/TAPT/checkpoint-100/config.json\n",
      "Model weights saved in ./results/TAPT/checkpoint-100/pytorch_model.bin\n",
      "Deleting older checkpoint [results/TAPT/checkpoint-1700] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/TAPT/checkpoint-200\n",
      "Configuration saved in ./results/TAPT/checkpoint-200/config.json\n",
      "Model weights saved in ./results/TAPT/checkpoint-200/pytorch_model.bin\n",
      "Deleting older checkpoint [results/TAPT/checkpoint-1800] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/TAPT/checkpoint-300\n",
      "Configuration saved in ./results/TAPT/checkpoint-300/config.json\n",
      "Model weights saved in ./results/TAPT/checkpoint-300/pytorch_model.bin\n",
      "Deleting older checkpoint [results/TAPT/checkpoint-1900] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/TAPT/checkpoint-400\n",
      "Configuration saved in ./results/TAPT/checkpoint-400/config.json\n",
      "Model weights saved in ./results/TAPT/checkpoint-400/pytorch_model.bin\n",
      "Deleting older checkpoint [results/TAPT/checkpoint-2000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/TAPT/checkpoint-500\n",
      "Configuration saved in ./results/TAPT/checkpoint-500/config.json\n",
      "Model weights saved in ./results/TAPT/checkpoint-500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/TAPT/checkpoint-2100] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/TAPT/checkpoint-600\n",
      "Configuration saved in ./results/TAPT/checkpoint-600/config.json\n",
      "Model weights saved in ./results/TAPT/checkpoint-600/pytorch_model.bin\n",
      "Deleting older checkpoint [results/TAPT/checkpoint-2200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/TAPT/checkpoint-700\n",
      "Configuration saved in ./results/TAPT/checkpoint-700/config.json\n",
      "Model weights saved in ./results/TAPT/checkpoint-700/pytorch_model.bin\n",
      "Deleting older checkpoint [results/TAPT/checkpoint-2300] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/TAPT/checkpoint-800\n",
      "Configuration saved in ./results/TAPT/checkpoint-800/config.json\n",
      "Model weights saved in ./results/TAPT/checkpoint-800/pytorch_model.bin\n",
      "Deleting older checkpoint [results/TAPT/checkpoint-2400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/TAPT/checkpoint-900\n",
      "Configuration saved in ./results/TAPT/checkpoint-900/config.json\n",
      "Model weights saved in ./results/TAPT/checkpoint-900/pytorch_model.bin\n",
      "Deleting older checkpoint [results/TAPT/checkpoint-2500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/TAPT/checkpoint-1000\n",
      "Configuration saved in ./results/TAPT/checkpoint-1000/config.json\n",
      "Model weights saved in ./results/TAPT/checkpoint-1000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/TAPT/checkpoint-2600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/TAPT/checkpoint-1100\n",
      "Configuration saved in ./results/TAPT/checkpoint-1100/config.json\n",
      "Model weights saved in ./results/TAPT/checkpoint-1100/pytorch_model.bin\n",
      "Deleting older checkpoint [results/TAPT/checkpoint-100] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/TAPT/checkpoint-1200\n",
      "Configuration saved in ./results/TAPT/checkpoint-1200/config.json\n",
      "Model weights saved in ./results/TAPT/checkpoint-1200/pytorch_model.bin\n",
      "Deleting older checkpoint [results/TAPT/checkpoint-200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/TAPT/checkpoint-1300\n",
      "Configuration saved in ./results/TAPT/checkpoint-1300/config.json\n",
      "Model weights saved in ./results/TAPT/checkpoint-1300/pytorch_model.bin\n",
      "Deleting older checkpoint [results/TAPT/checkpoint-300] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/TAPT/checkpoint-1400\n",
      "Configuration saved in ./results/TAPT/checkpoint-1400/config.json\n",
      "Model weights saved in ./results/TAPT/checkpoint-1400/pytorch_model.bin\n",
      "Deleting older checkpoint [results/TAPT/checkpoint-400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/TAPT/checkpoint-1500\n",
      "Configuration saved in ./results/TAPT/checkpoint-1500/config.json\n",
      "Model weights saved in ./results/TAPT/checkpoint-1500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/TAPT/checkpoint-500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/TAPT/checkpoint-1600\n",
      "Configuration saved in ./results/TAPT/checkpoint-1600/config.json\n",
      "Model weights saved in ./results/TAPT/checkpoint-1600/pytorch_model.bin\n",
      "Deleting older checkpoint [results/TAPT/checkpoint-600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/TAPT/checkpoint-1700\n",
      "Configuration saved in ./results/TAPT/checkpoint-1700/config.json\n",
      "Model weights saved in ./results/TAPT/checkpoint-1700/pytorch_model.bin\n",
      "Deleting older checkpoint [results/TAPT/checkpoint-700] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/TAPT/checkpoint-1800\n",
      "Configuration saved in ./results/TAPT/checkpoint-1800/config.json\n",
      "Model weights saved in ./results/TAPT/checkpoint-1800/pytorch_model.bin\n",
      "Deleting older checkpoint [results/TAPT/checkpoint-800] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/TAPT/checkpoint-1900\n",
      "Configuration saved in ./results/TAPT/checkpoint-1900/config.json\n",
      "Model weights saved in ./results/TAPT/checkpoint-1900/pytorch_model.bin\n",
      "Deleting older checkpoint [results/TAPT/checkpoint-900] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/TAPT/checkpoint-2000\n",
      "Configuration saved in ./results/TAPT/checkpoint-2000/config.json\n",
      "Model weights saved in ./results/TAPT/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/TAPT/checkpoint-1000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/TAPT/checkpoint-2100\n",
      "Configuration saved in ./results/TAPT/checkpoint-2100/config.json\n",
      "Model weights saved in ./results/TAPT/checkpoint-2100/pytorch_model.bin\n",
      "Deleting older checkpoint [results/TAPT/checkpoint-1100] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/TAPT/checkpoint-2200\n",
      "Configuration saved in ./results/TAPT/checkpoint-2200/config.json\n",
      "Model weights saved in ./results/TAPT/checkpoint-2200/pytorch_model.bin\n",
      "Deleting older checkpoint [results/TAPT/checkpoint-1200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/TAPT/checkpoint-2300\n",
      "Configuration saved in ./results/TAPT/checkpoint-2300/config.json\n",
      "Model weights saved in ./results/TAPT/checkpoint-2300/pytorch_model.bin\n",
      "Deleting older checkpoint [results/TAPT/checkpoint-1300] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/TAPT/checkpoint-2400\n",
      "Configuration saved in ./results/TAPT/checkpoint-2400/config.json\n",
      "Model weights saved in ./results/TAPT/checkpoint-2400/pytorch_model.bin\n",
      "Deleting older checkpoint [results/TAPT/checkpoint-1400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/TAPT/checkpoint-2500\n",
      "Configuration saved in ./results/TAPT/checkpoint-2500/config.json\n",
      "Model weights saved in ./results/TAPT/checkpoint-2500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/TAPT/checkpoint-1500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/TAPT/checkpoint-2600\n",
      "Configuration saved in ./results/TAPT/checkpoint-2600/config.json\n",
      "Model weights saved in ./results/TAPT/checkpoint-2600/pytorch_model.bin\n",
      "Deleting older checkpoint [results/TAPT/checkpoint-1600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/TAPT/checkpoint-2700\n",
      "Configuration saved in ./results/TAPT/checkpoint-2700/config.json\n",
      "Model weights saved in ./results/TAPT/checkpoint-2700/pytorch_model.bin\n",
      "Deleting older checkpoint [results/TAPT/checkpoint-1700] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/TAPT/checkpoint-2800\n",
      "Configuration saved in ./results/TAPT/checkpoint-2800/config.json\n",
      "Model weights saved in ./results/TAPT/checkpoint-2800/pytorch_model.bin\n",
      "Deleting older checkpoint [results/TAPT/checkpoint-1800] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/TAPT/checkpoint-2900\n",
      "Configuration saved in ./results/TAPT/checkpoint-2900/config.json\n",
      "Model weights saved in ./results/TAPT/checkpoint-2900/pytorch_model.bin\n",
      "Deleting older checkpoint [results/TAPT/checkpoint-1900] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/TAPT/checkpoint-3000\n",
      "Configuration saved in ./results/TAPT/checkpoint-3000/config.json\n",
      "Model weights saved in ./results/TAPT/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/TAPT/checkpoint-2000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results/TAPT/checkpoint-2500 (score: 2.090914487838745).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish, PID 4083292... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 12.69MB of 12.69MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">Task Adaptive Pretraining</strong>: <a href=\"https://wandb.ai/emandai/intent-classifier/runs/3pblif3k\" target=\"_blank\">https://wandb.ai/emandai/intent-classifier/runs/3pblif3k</a><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "wandb.finish(quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8208c95-5050-43ee-b0fc-5d1a382b1d29",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "40c3116b-5d27-49fd-ad7d-c02024a4268c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./results/TAPT/checkpoint-2500/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"vinai/phobert-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 258,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"PhobertTokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n",
      "loading weights file ./results/TAPT/checkpoint-2500/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForMaskedLM.\n",
      "\n",
      "All the weights of RobertaForMaskedLM were initialized from the model checkpoint at ./results/TAPT/checkpoint-2500.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.\n",
      "loading configuration file https://huggingface.co/vinai/phobert-base/resolve/main/config.json from cache at /home/kiethoang/.cache/huggingface/transformers/a596f267f08b7158c7ab6300b1bf98eb6e1b05e6bcb0d7c18a8070364ee3011b.bbe27b2cac909b2279c83792c2d2b6f159f0a95f5d1c1eb66451da1c89a53609\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 258,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"PhobertTokenizer\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/vinai/phobert-base/resolve/main/pytorch_model.bin from cache at /home/kiethoang/.cache/huggingface/transformers/8363542cfd9e2bad1a9a618e87ea1153d84819a3ae581cff0816a2c1f610f433.42a5e558f15db4cc3af338445707272b8f7545df78efdc125d3fd51025b22d85\n",
      "All model checkpoint weights were used when initializing RobertaForMaskedLM.\n",
      "\n",
      "All the weights of RobertaForMaskedLM were initialized from the model checkpoint at vinai/phobert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/vinai/phobert-base/resolve/main/config.json from cache at /home/kiethoang/.cache/huggingface/transformers/a596f267f08b7158c7ab6300b1bf98eb6e1b05e6bcb0d7c18a8070364ee3011b.bbe27b2cac909b2279c83792c2d2b6f159f0a95f5d1c1eb66451da1c89a53609\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 258,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"PhobertTokenizer\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/vinai/phobert-base/resolve/main/vocab.txt from cache at /home/kiethoang/.cache/huggingface/transformers/970c6224b2713c8b52a7bcfc4d5a951c9bb88302e4523388b50f28284e87ac44.26ba0c8945e559c68d0bc35d24fea16f5463a49fe8f134e0c32261d590b577fa\n",
      "loading file https://huggingface.co/vinai/phobert-base/resolve/main/bpe.codes from cache at /home/kiethoang/.cache/huggingface/transformers/f3a66ae0a78d1a53b3eb99e31837d0d8e2f684a2dcc1f52f75fd36873e3d79de.301ac8958de708ddcea8500d9acbe6261dba391d249c98dcda1e49dbbff870dd\n",
      "loading file https://huggingface.co/vinai/phobert-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/vinai/phobert-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/vinai/phobert-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading file https://huggingface.co/vinai/phobert-base/resolve/main/tokenizer.json from cache at None\n",
      "loading configuration file https://huggingface.co/vinai/phobert-base/resolve/main/config.json from cache at /home/kiethoang/.cache/huggingface/transformers/a596f267f08b7158c7ab6300b1bf98eb6e1b05e6bcb0d7c18a8070364ee3011b.bbe27b2cac909b2279c83792c2d2b6f159f0a95f5d1c1eb66451da1c89a53609\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 258,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"PhobertTokenizer\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n",
      "Adding <mask> to the vocabulary\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" MLM TESTING \"\"\"\n",
    "\n",
    "# Load pretrained model\n",
    "emandai_model = AutoModelForMaskedLM.from_pretrained(\"./results/TAPT/checkpoint-2500\")\n",
    "vinai_model = AutoModelForMaskedLM.from_pretrained(phobert_base_checkpoint)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(phobert_base_checkpoint, use_fast=False)\n",
    "\n",
    "emandai_unmasker = pipeline(\"fill-mask\", model=emandai_model, tokenizer=tokenizer)\n",
    "vinai_unmasker = pipeline(\"fill-mask\", model=vinai_model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "17096dac-cda9-4186-af01-437ff9e1f424",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForMaskedLM(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(64001, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(258, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): RobertaLMHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (decoder): Linear(in_features=768, out_features=64001, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emandai_model.eval()\n",
    "vinai_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "df71ec42-3150-4b10-bbde-6306aea2e907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lấy tiền gửi cho cháu đi học thêm'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" MLM TESTING \"\"\"\n",
    "\n",
    "text = _ws(\"lấy tiền gửi cho cháu đi học thêm\")\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9212e4d2-9d7d-4ec0-be4c-6e1698299dbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': 'lấy tiền gửi cho cháu đi học thêm',\n",
       "  'score': 0.6051039099693298,\n",
       "  'token': 143,\n",
       "  'token_str': 't h ê m'},\n",
       " {'sequence': 'lấy tiền gửi cho cháu đi học đại_học',\n",
       "  'score': 0.1962745040655136,\n",
       "  'token': 956,\n",
       "  'token_str': 'đ ạ i _ h ọ c'},\n",
       " {'sequence': 'lấy tiền gửi cho cháu đi học ngoại_ngữ',\n",
       "  'score': 0.05282137915492058,\n",
       "  'token': 4408,\n",
       "  'token_str': 'n g o ạ i _ n g ữ'},\n",
       " {'sequence': 'lấy tiền gửi cho cháu đi học nữa',\n",
       "  'score': 0.03260941058397293,\n",
       "  'token': 348,\n",
       "  'token_str': 'n ữ a'},\n",
       " {'sequence': 'lấy tiền gửi cho cháu đi học nước_ngoài',\n",
       "  'score': 0.01431557722389698,\n",
       "  'token': 516,\n",
       "  'token_str': 'n ư ớ c _ n g o à i'}]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emandai_unmasker(\"lấy tiền gửi cho cháu đi học <mask>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a5c14d98-f42f-4df9-86fe-8cde19371742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': 'lấy tiền gửi cho cháu đi học.',\n",
       "  'score': 0.9702707529067993,\n",
       "  'token': 5,\n",
       "  'token_str': '.'},\n",
       " {'sequence': 'lấy tiền gửi cho cháu đi học :',\n",
       "  'score': 0.013175510801374912,\n",
       "  'token': 27,\n",
       "  'token_str': ':'},\n",
       " {'sequence': 'lấy tiền gửi cho cháu đi học ;',\n",
       "  'score': 0.006559078581631184,\n",
       "  'token': 65,\n",
       "  'token_str': ';'},\n",
       " {'sequence': 'lấy tiền gửi cho cháu đi học...',\n",
       "  'score': 0.006110189016908407,\n",
       "  'token': 135,\n",
       "  'token_str': '...'},\n",
       " {'sequence': 'lấy tiền gửi cho cháu đi học?',\n",
       "  'score': 0.00104904908221215,\n",
       "  'token': 114,\n",
       "  'token_str': '?'}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vinai_unmasker(\"lấy tiền gửi cho cháu đi học <mask>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19501885-d063-464a-95b5-07b3484ce318",
   "metadata": {
    "tags": []
   },
   "source": [
    "## SVM BASELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "630f8864-effd-4e48-b433-7af96c49ec06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pformat\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6b4b7b99-e17b-48c9-978a-7b1d727e38d0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.733\n",
      "\n",
      "Best params:\n",
      " {'clf__estimator__C': 0.1,\n",
      " 'clf__estimator__class_weight': 'balanced',\n",
      " 'vect__max_df': 0.5,\n",
      " 'vect__ngram_range': (1, 1)}\n",
      "\n",
      "Best estimator:\n",
      " Pipeline(steps=[('vect', TfidfVectorizer(max_df=0.5)),\n",
      "                ('scaler', MaxAbsScaler()),\n",
      "                ('clf',\n",
      "                 OneVsRestClassifier(estimator=LinearSVC(C=0.1,\n",
      "                                                         class_weight='balanced',\n",
      "                                                         random_state=1337),\n",
      "                                     n_jobs=-1))])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    (\"vect\", TfidfVectorizer()),\n",
    "    (\"scaler\", MaxAbsScaler()),\n",
    "    (\"clf\", OneVsRestClassifier(LinearSVC(random_state=1337), n_jobs=-1))\n",
    "])\n",
    "grid = [{\n",
    "        \"vect__ngram_range\": [(1, 1), (1, 2), (2, 2)],\n",
    "        \"vect__max_df\": [0.2, 0.5, 0.75, 1.0],\n",
    "        \"clf__estimator__C\": [0.01, 0.1, 1, 10],\n",
    "        \"clf__estimator__class_weight\": [\"balanced\", None]\n",
    "}]\n",
    "\n",
    "gridcv = GridSearchCV(estimator=pipeline,\n",
    "                      param_grid=grid,\n",
    "                      cv=5,\n",
    "                      scoring=\"f1_macro\",\n",
    "                      n_jobs=-1,\n",
    "                      return_train_score=True)\n",
    "gridcv.fit([*train_texts, *val_texts], [*train_labels, *val_labels])\n",
    "\n",
    "# Statistic verbose\n",
    "print(f\"Best score: {gridcv.best_score_:.3f}\", end=\"\\n\\n\")\n",
    "print(f\"Best params:\\n {pformat(gridcv.best_params_)}\", end=\"\\n\\n\")\n",
    "print(f\"Best estimator:\\n {pformat(gridcv.best_estimator_)}\", end=\"\\n\\n\")\n",
    "\n",
    "linear_svm = gridcv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9fc1950c-7ba2-4b6f-98e4-33a6410cd85a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 macro [TEST SET]: 0.6370415684180867\n"
     ]
    }
   ],
   "source": [
    "def _evaluate(model, X, y):\n",
    "    y_preds = model.predict(X)\n",
    "    f1_scores = f1_score(y, y_preds, average=\"macro\")\n",
    "    return f1_scores\n",
    "\n",
    "# Evalute on dev set\n",
    "# preds = linear_svm.predict(val_texts)\n",
    "# f1_scores = f1_score(val_labels, preds, average=\"macro\")\n",
    "# dev_f1_score = _evaluate(linear_svm, val_texts, val_labels)\n",
    "# print(f\"F1 macro [DEV SET]: {dev_f1_score}\")\n",
    "\n",
    "# Evalute on test set\n",
    "test_f1_score = _evaluate(linear_svm, test_texts, test_labels)\n",
    "print(f\"F1 macro [TEST SET]: {test_f1_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "60fc83c7-7520-4d3b-96a7-ba8728bed220",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 macro [REAL TEST SET]: 0.3203896073664704\n"
     ]
    }
   ],
   "source": [
    "# Evalute on REAL test set\n",
    "real_test_f1_score = _evaluate(linear_svm, X_test, y_test)\n",
    "print(f\"F1 macro [REAL TEST SET]: {real_test_f1_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf300b4-94d5-4db9-b910-6921e50c6229",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" CONFUSION MATRIX [SVM] \"\"\"\n",
    "\n",
    "y_preds = linear_svm.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_preds)\n",
    "\n",
    "print(len(np.unique(y_preds)))\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a5e6a8-2a1d-4955-b921-fef70f21a04a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
