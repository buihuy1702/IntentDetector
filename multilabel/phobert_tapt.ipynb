{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38a5dfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import math\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForMaskedLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TrainingArguments\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import Trainer\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54179b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"./data/telesale/100_calls.xlsx\", engine=\"openpyxl\", sheet_name=\"100 calls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d92d1db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fileName</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>speaker</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>Intent</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "      <th>Unnamed: 8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VUQC83EOO969H66VRLO5RMEPC8089Q00_2021-07-16_11...</td>\n",
       "      <td>0.81</td>\n",
       "      <td>1.27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>a lô</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.86</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>anh đơn hả anh</td>\n",
       "      <td>Agent_VerifyCustomerName</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.62</td>\n",
       "      <td>7.86</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>ờ anh em là hương nè em gọi đến cho anh từ bên...</td>\n",
       "      <td>Agent_Self Introduction</td>\n",
       "      <td>Agent_CompanyIntroduction</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>8.04</td>\n",
       "      <td>34.32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>a lô cho em hỏi xíu ha là vợ anh chị lép á còn...</td>\n",
       "      <td>Agent_VerifyCustomerName</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>34.68</td>\n",
       "      <td>35.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>anh anh</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            fileName  start    end  speaker  \\\n",
       "0  VUQC83EOO969H66VRLO5RMEPC8089Q00_2021-07-16_11...   0.81   1.27      0.0   \n",
       "1                                                NaN   1.86   2.76      0.0   \n",
       "2                                                NaN   4.62   7.86      0.0   \n",
       "3                                                NaN   8.04  34.32      0.0   \n",
       "4                                                NaN  34.68  35.07      0.0   \n",
       "\n",
       "   sentiment                                               text  \\\n",
       "0        3.0                                               a lô   \n",
       "1        3.0                                     anh đơn hả anh   \n",
       "2        3.0  ờ anh em là hương nè em gọi đến cho anh từ bên...   \n",
       "3        3.0  a lô cho em hỏi xíu ha là vợ anh chị lép á còn...   \n",
       "4        3.0                                            anh anh   \n",
       "\n",
       "                     Intent                 Unnamed: 7 Unnamed: 8  \n",
       "0                       NaN                        NaN        NaN  \n",
       "1  Agent_VerifyCustomerName                        NaN        NaN  \n",
       "2   Agent_Self Introduction  Agent_CompanyIntroduction        NaN  \n",
       "3  Agent_VerifyCustomerName                        NaN        NaN  \n",
       "4                       NaN                        NaN        NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f62cacd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a lô',\n",
      " 'anh đơn hả anh',\n",
      " 'ờ anh em là hương nè em gọi đến cho anh từ bên ngân hàng vi bi banh á',\n",
      " 'a lô cho em hỏi xíu ha là vợ anh chị lép á còn dùng số điện thoại cũ không à '\n",
      " 'em hỏi là vợ anh mà chị lép á anh còn dùng số điện thoại cũ không anh ha số '\n",
      " 'bốn tám bốn đuôi á đổi số rồi hả ủa ba không hai anh ơi anh đọc giùm em được '\n",
      " 'không à vậy ạ',\n",
      " 'anh anh',\n",
      " 'ừm',\n",
      " 'anh đưa điện thoại giùm em',\n",
      " nan,\n",
      " 'à chị lép hả chị',\n",
      " 'à hiện tại hiện thì trong cái đợt dịch đợt dịch này nè em à cũng bên em cũng '\n",
      " 'có đang hỗ trợ lại cho khách hàng cũ á']\n",
      "9368\n"
     ]
    }
   ],
   "source": [
    "# Just printing\n",
    "texts = df[\"text\"].tolist()\n",
    "pprint(texts[:10])\n",
    "print(len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00468251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a lô',\n",
      " 'anh đơn hả anh',\n",
      " 'ờ anh em là hương nè em gọi đến cho anh từ bên ngân hàng vi bi banh á',\n",
      " 'a lô cho em hỏi xíu ha là vợ anh chị lép á còn dùng số điện thoại cũ không à '\n",
      " 'em hỏi là vợ anh mà chị lép á anh còn dùng số điện thoại cũ không anh ha số '\n",
      " 'bốn tám bốn đuôi á đổi số rồi hả ủa ba không hai anh ơi anh đọc giùm em được '\n",
      " 'không à vậy ạ',\n",
      " 'anh anh',\n",
      " 'ừm',\n",
      " 'anh đưa điện thoại giùm em',\n",
      " 'à chị lép hả chị',\n",
      " 'à hiện tại hiện thì trong cái đợt dịch đợt dịch này nè em à cũng bên em cũng '\n",
      " 'có đang hỗ trợ lại cho khách hàng cũ á',\n",
      " 'cái điện số điện thoại cũ của chị thì em gọi nó mấy nay không có được á nên '\n",
      " 'là xin phép em nói sơ qua chút xíu ha']\n",
      "8614\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Remove nan \"\"\"\n",
    "texts = [text for text in texts if isinstance(text, str)]\n",
    "pprint(texts[:10])\n",
    "print(len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0e4466c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dạ à',\n",
      " 'cho nên em cứ cân nhắc đi rồi có gì tuần sau chị gọi lại cho em',\n",
      " 'thứ hai là nó có hợp đồng đàng hoàng cho chị mà chứ đâu phải nói chuyện '\n",
      " 'không không ví dụ à em nói em cho chị vay như vậy không đâu đâu có',\n",
      " 'thì tiền mặt luôn cái ốp bồ mới ra nó cứ nó cứ điện thoại gì cũng mới ra '\n",
      " 'cũng đây mấy tháng á hai ba tháng nè',\n",
      " 'ờ rồi',\n",
      " 'trời ơi tâm thần của mình không ăn uống gì được nữa trời ơi',\n",
      " 'ừ mà họ không duyệt nữa là thôi',\n",
      " 'khi mà mình có uy tín của ngân hàng rồi thì mình mới vay được ờ',\n",
      " 'thì có anh có đóng tốn hai lăm ngàn không',\n",
      " 'hồ sơ của chị chị được duyệt á em đăng ký cho mình thì chị chỉ cần cầm giấy '\n",
      " 'chứng minh ra ngân hàng']\n",
      "6519\n"
     ]
    }
   ],
   "source": [
    "texts = list(set(texts))\n",
    "pprint(texts[:10])\n",
    "print(len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94372c30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 6519\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset.from_dict({\"text\": texts})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c8938c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9235bb97efc342119b871b1445cd61b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n",
    "\n",
    "def tokenize(samples):\n",
    "    return tokenizer(samples[\"text\"], return_special_tokens_mask=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d5c5c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.2, seed=RANDOM_SEED)\n",
    "trainset = tokenized_dataset[\"train\"]\n",
    "evalset = tokenized_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02c2f513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'special_tokens_mask', 'attention_mask'],\n",
       "    num_rows: 5215\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c39d39b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'special_tokens_mask', 'attention_mask'],\n",
       "    num_rows: 1304\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evalset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3cc354cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaConfig {\n",
       "  \"_name_or_path\": \"vinai/phobert-base\",\n",
       "  \"architectures\": [\n",
       "    \"RobertaForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-05,\n",
       "  \"max_position_embeddings\": 258,\n",
       "  \"model_type\": \"roberta\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"tokenizer_class\": \"PhobertTokenizer\",\n",
       "  \"transformers_version\": \"4.12.5\",\n",
       "  \"type_vocab_size\": 1,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 64001\n",
       "}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForMaskedLM.from_pretrained(\"vinai/phobert-base\")\n",
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a7815c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    num_train_epochs=60,\n",
    "    learning_rate=1e-4,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.06,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=2,\n",
    "    logging_steps=100,\n",
    "    logging_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_total_limit=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\",\n",
    "    logging_dir=\"./logs/tapt/telesale\",\n",
    "    output_dir=\"./results/tapt/telesale\",\n",
    "    report_to=\"wandb\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0614e0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1101786",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=trainset,\n",
    "    eval_dataset=evalset,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ebaa9c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running training *****\n",
      "  Num examples = 5215\n",
      "  Num Epochs = 60\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 9780\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhosjiu\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/buihuy/Intent-Detection/multilabel/wandb/run-20220307_130326-2ooom20x</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/hosjiu/huggingface/runs/2ooom20x\" target=\"_blank\">./results/tapt/telesale</a></strong> to <a href=\"https://wandb.ai/hosjiu/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9780' max='9780' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9780/9780 22:36, Epoch 60/60]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>4.399200</td>\n",
       "      <td>3.611763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.466000</td>\n",
       "      <td>3.174999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.113100</td>\n",
       "      <td>2.914603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.863300</td>\n",
       "      <td>2.885626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.867100</td>\n",
       "      <td>2.785375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.697400</td>\n",
       "      <td>2.752808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.640800</td>\n",
       "      <td>2.614743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.551700</td>\n",
       "      <td>2.494638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.467900</td>\n",
       "      <td>2.625203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.485600</td>\n",
       "      <td>2.539428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>2.321600</td>\n",
       "      <td>2.583907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.341500</td>\n",
       "      <td>2.540837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>2.266400</td>\n",
       "      <td>2.401746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.195600</td>\n",
       "      <td>2.457790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.074000</td>\n",
       "      <td>2.545462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.034300</td>\n",
       "      <td>2.393173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>2.005000</td>\n",
       "      <td>2.424242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>2.058400</td>\n",
       "      <td>2.411400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>1.883400</td>\n",
       "      <td>2.435878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.907300</td>\n",
       "      <td>2.427466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>1.908400</td>\n",
       "      <td>2.448438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>1.835900</td>\n",
       "      <td>2.396230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>1.829700</td>\n",
       "      <td>2.505155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>1.779100</td>\n",
       "      <td>2.406223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.770400</td>\n",
       "      <td>2.362286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>1.738200</td>\n",
       "      <td>2.471511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>1.718100</td>\n",
       "      <td>2.297765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>1.671900</td>\n",
       "      <td>2.417017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>1.559600</td>\n",
       "      <td>2.365282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.603100</td>\n",
       "      <td>2.455921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>1.726100</td>\n",
       "      <td>2.368186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>1.589200</td>\n",
       "      <td>2.440696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>1.675700</td>\n",
       "      <td>2.398132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>1.567500</td>\n",
       "      <td>2.327645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.474200</td>\n",
       "      <td>2.320305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>1.520200</td>\n",
       "      <td>2.327832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>1.388400</td>\n",
       "      <td>2.358077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>1.418600</td>\n",
       "      <td>2.290595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>1.396100</td>\n",
       "      <td>2.401047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.344100</td>\n",
       "      <td>2.446458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>1.386200</td>\n",
       "      <td>2.405500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>1.363100</td>\n",
       "      <td>2.392738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>1.306700</td>\n",
       "      <td>2.355386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>1.325000</td>\n",
       "      <td>2.312578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.247800</td>\n",
       "      <td>2.314002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>1.255800</td>\n",
       "      <td>2.395326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>1.258200</td>\n",
       "      <td>2.377762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>1.171600</td>\n",
       "      <td>2.254889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>1.192800</td>\n",
       "      <td>2.356761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.142600</td>\n",
       "      <td>2.376198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>1.141900</td>\n",
       "      <td>2.329227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>1.143500</td>\n",
       "      <td>2.378663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>1.122200</td>\n",
       "      <td>2.287780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>1.106700</td>\n",
       "      <td>2.265708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>1.067600</td>\n",
       "      <td>2.367640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>1.027800</td>\n",
       "      <td>2.546584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>1.047000</td>\n",
       "      <td>2.450499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>1.013900</td>\n",
       "      <td>2.482688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>1.033600</td>\n",
       "      <td>2.434040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.015900</td>\n",
       "      <td>2.410644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>0.991000</td>\n",
       "      <td>2.428608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>1.002000</td>\n",
       "      <td>2.621034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>0.992700</td>\n",
       "      <td>2.451071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.958100</td>\n",
       "      <td>2.412398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.923900</td>\n",
       "      <td>2.504545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.881200</td>\n",
       "      <td>2.434232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>0.870200</td>\n",
       "      <td>2.393066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.938700</td>\n",
       "      <td>2.422433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>0.850700</td>\n",
       "      <td>2.481011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.872600</td>\n",
       "      <td>2.533941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>0.819600</td>\n",
       "      <td>2.364285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.814300</td>\n",
       "      <td>2.428403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>0.803000</td>\n",
       "      <td>2.501690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.818500</td>\n",
       "      <td>2.541372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.799800</td>\n",
       "      <td>2.462907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>0.806500</td>\n",
       "      <td>2.490028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>0.761500</td>\n",
       "      <td>2.459883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>0.783600</td>\n",
       "      <td>2.359247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>0.739200</td>\n",
       "      <td>2.473424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.742400</td>\n",
       "      <td>2.410619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>0.725800</td>\n",
       "      <td>2.541873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>0.723600</td>\n",
       "      <td>2.514750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>0.763200</td>\n",
       "      <td>2.443488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>0.714000</td>\n",
       "      <td>2.503650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.653800</td>\n",
       "      <td>2.469699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>0.666200</td>\n",
       "      <td>2.472633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8700</td>\n",
       "      <td>0.683700</td>\n",
       "      <td>2.402016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>0.666900</td>\n",
       "      <td>2.481652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8900</td>\n",
       "      <td>0.657600</td>\n",
       "      <td>2.457261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.656400</td>\n",
       "      <td>2.537108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9100</td>\n",
       "      <td>0.678200</td>\n",
       "      <td>2.585546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>0.642300</td>\n",
       "      <td>2.559699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9300</td>\n",
       "      <td>0.703600</td>\n",
       "      <td>2.442965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>0.662600</td>\n",
       "      <td>2.432449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.693400</td>\n",
       "      <td>2.401851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>0.665000</td>\n",
       "      <td>2.521151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9700</td>\n",
       "      <td>0.618800</td>\n",
       "      <td>2.473494</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-100\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-100/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-100/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-200\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-200/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-200/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-300\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-300/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-300/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-400\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-400/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-400/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-500\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-500/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-600\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-600/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-600/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-700\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-700/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-700/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-800\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-800/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-800/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-900\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-900/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-900/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-1000\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-1000/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-1000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-1100\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-1100/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-1100/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-100] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-1200\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-1200/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-1200/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-200] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-1300\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-1300/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-1300/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-300] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-1400\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-1400/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-1400/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-400] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-1500\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-1500/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-1500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-1600\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-1600/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-1600/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-600] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-1700\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-1700/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-1700/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-700] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-1800\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-1800/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-1800/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-800] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-1900\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-1900/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-1900/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-900] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-2000\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-2000/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-1000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-2100\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-2100/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-2100/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-1100] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-2200\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-2200/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-2200/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-1200] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-2300\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-2300/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-2300/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-1300] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-2400\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-2400/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-2400/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-1400] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-2500\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-2500/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-2500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-1500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-2600\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-2600/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-2600/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-1600] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-2700\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-2700/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-2700/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-1700] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-2800\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-2800/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-2800/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-1800] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-2900\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-2900/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-2900/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-1900] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-3000\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-3000/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-2000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-3100\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-3100/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-3100/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-2100] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-3200\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-3200/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-3200/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-2200] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-3300\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-3300/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-3300/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-2300] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-3400\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-3400/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-3400/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-2400] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-3500\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-3500/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-3500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-2500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-3600\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-3600/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-3600/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-2600] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-3700\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-3700/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-3700/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-2800] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-3800\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-3800/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-3800/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-2700] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-3900\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-3900/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-3900/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-2900] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-4000\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-4000/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-3000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-4100\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-4100/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-4100/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-3100] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-4200\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-4200/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-4200/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-3200] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-4300\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-4300/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-4300/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-3300] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-4400\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-4400/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-4400/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-3400] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-4500\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-4500/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-4500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-3500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-4600\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-4600/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-4600/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-3600] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-4700\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-4700/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-4700/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-3700] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-4800\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-4800/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-4800/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-3800] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-4900\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-4900/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-4900/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-3900] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-5000\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-5000/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-5000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-4000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-5100\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-5100/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-5100/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-4100] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-5200\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-5200/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-5200/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-4200] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-5300\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-5300/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-5300/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-4300] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-5400\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-5400/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-5400/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-4400] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-5500\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-5500/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-5500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-4500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-5600\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-5600/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-5600/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-4600] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-5700\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-5700/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-5700/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-4700] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-5800\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-5800/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-5800/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-4900] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-5900\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-5900/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-5900/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-5000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-6000\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-6000/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-6000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-5100] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-6100\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-6100/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-6100/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-5200] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-6200\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-6200/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-6200/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-5300] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-6300\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-6300/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-6300/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-5400] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-6400\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-6400/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-6400/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-5500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-6500\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-6500/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-6500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-5600] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-6600\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-6600/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-6600/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-5700] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-6700\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-6700/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-6700/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-5800] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-6800\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-6800/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-6800/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-5900] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-6900\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-6900/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-6900/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-6000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-7000\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-7000/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-7000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-6100] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-7100\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-7100/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-7100/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-6200] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-7200\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-7200/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-7200/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-6300] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-7300\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-7300/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-7300/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-6400] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-7400\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-7400/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-7400/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-6500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-7500\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-7500/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-7500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-6600] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-7600\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-7600/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-7600/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-6700] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-7700\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-7700/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-7700/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-6800] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-7800\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-7800/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-7800/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-6900] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-7900\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-7900/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-7900/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-7000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-8000\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-8000/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-8000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-7100] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-8100\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-8100/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-8100/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-7200] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-8200\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-8200/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-8200/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-7300] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-8300\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-8300/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-8300/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-7400] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-8400\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-8400/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-8400/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-7500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-8500\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-8500/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-8500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-7600] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-8600\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-8600/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-8600/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-7700] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-8700\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-8700/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-8700/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-7800] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-8800\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-8800/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-8800/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-7900] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-8900\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-8900/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-8900/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-8000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-9000\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-9000/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-9000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-8100] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-9100\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-9100/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-9100/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-8200] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-9200\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-9200/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-9200/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-8300] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-9300\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-9300/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-9300/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-8400] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-9400\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-9400/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-9400/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-8500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-9500\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-9500/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-9500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-8600] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-9600\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-9600/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-9600/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-8700] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1304\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/tapt/telesale/checkpoint-9700\n",
      "Configuration saved in ./results/tapt/telesale/checkpoint-9700/config.json\n",
      "Model weights saved in ./results/tapt/telesale/checkpoint-9700/pytorch_model.bin\n",
      "Deleting older checkpoint [results/tapt/telesale/checkpoint-8800] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results/tapt/telesale/checkpoint-4800 (score: 2.2548885345458984).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=9780, training_loss=1.3886506632549387, metrics={'train_runtime': 1358.795, 'train_samples_per_second': 230.278, 'train_steps_per_second': 7.198, 'total_flos': 7806330031926318.0, 'train_loss': 1.3886506632549387, 'epoch': 60.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
