{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cba8e5b4-3a8d-44c2-a782-ef84463282b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    AdamW, get_scheduler\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch\n",
    "from torch import cuda\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64bce39d-ad09-4a9e-b466-1f418a8d42fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  \\\n",
       "0  Explanation\\nWhy the edits made under my usern...   \n",
       "1  D'aww! He matches this background colour I'm s...   \n",
       "2  Hey man, I'm really not trying to edit war. It...   \n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...   \n",
       "4  You, sir, are my hero. Any chance you remember...   \n",
       "\n",
       "                           labels  \n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]  \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]  \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]  \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]  \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Label formatting\n",
    "\"\"\"\n",
    "labels = []\n",
    "label2id = {}\n",
    "id2label = {}\n",
    "\n",
    "df = pd.read_csv(\"./data/train.csv\")\n",
    "\n",
    "labels = df.columns.tolist()[2:]\n",
    "label2id = {label: idx for idx, label in enumerate(labels)}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "\n",
    "df[\"labels\"] = df[df.columns[2:]].values.astype(\"float\").tolist()\n",
    "df = df[[\"comment_text\", \"labels\"]].drop(df.index[100000:]).reset_index(drop=True)\n",
    "# df = df[[\"comment_text\", \"labels\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a586d2d-d47c-4809-bb54-fe4dd254ad02",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Load Huggingface dataset 3from In-memory data\n",
    "\"\"\"\n",
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37104a5e-df69-4481-bcdb-15fd1d3be702",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Encoding dataset (TOKENIZATION) for later use with BERT model\n",
    " - input_ids\n",
    " - attention_mask\n",
    " - token_type_ids\n",
    "\"\"\"\n",
    "MAX_LEN = 200\n",
    "tokenizer_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_checkpoint)\n",
    "\n",
    "def preprocess(samples):\n",
    "    \"\"\" \n",
    "        * Tokenize text(s) with a given pretrained tokenizer\n",
    "        * Casting targets type from int type to float\n",
    "    \"\"\"    \n",
    "    tokenized_samples = tokenizer(\n",
    "        text=samples[\"comment_text\"],\n",
    "        max_length=MAX_LEN,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_token_type_ids=True)\n",
    "\n",
    "    return tokenized_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "14b7e324-c360-44e8-a065-e77591d251fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Tokenize texts in dataset\n",
    "\"\"\"\n",
    "encoded_dataset = dataset.map(function=preprocess,\n",
    "                              batched=True,\n",
    "                              num_proc=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0cdd0be6-109d-4d8e-81db-81ab4fad7a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Train/Test split\n",
    "\"\"\"\n",
    "TEST_SIZE = 0.2\n",
    "SEED = 42\n",
    "encoded_dataset = encoded_dataset.train_test_split(\n",
    "                            test_size=TEST_SIZE,\n",
    "                            seed=SEED)\n",
    "train_dataset = encoded_dataset[\"train\"].remove_columns([\"comment_text\"])\n",
    "val_dataset = encoded_dataset[\"test\"].remove_columns([\"comment_text\"])\n",
    "\n",
    "# Set output format\n",
    "train_dataset.set_format(\"torch\")\n",
    "val_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "626759c1-0ccb-487d-9495-d302aedc5c03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" Create Train/Test Dataloader (use in training loop)\n",
    "\"\"\"\n",
    "BATCH_SIZE = 16\n",
    "train_dataloader = DataLoader(dataset=train_dataset,\n",
    "                              shuffle=True,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              num_workers=4)\n",
    "val_dataloader = DataLoader(dataset=val_dataset,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d63b39de-ca38-4d94-bab3-49029a438261",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Model Definition\n",
    "\"\"\"\n",
    "LR = 5e-5 # learning rate\n",
    "N_EPOCHS = 3\n",
    "N_TRAIN_STEPS = N_EPOCHS * len(train_dataloader)\n",
    "\n",
    "# Define model (with multi-label configuration)\n",
    "model_ckpt = \"bert-base-cased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_ckpt,\n",
    "    problem_type=\"multi_label_classification\",\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    num_labels=len(labels)\n",
    ")\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(params=model.parameters(), lr=LR)\n",
    "\n",
    "# LR scheduler\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=N_TRAIN_STEPS)\n",
    "\n",
    "# CPU/GPU switching\n",
    "device = torch.device(\"cuda:0\") if cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4dc90070-bd43-4f01-bbd7-39e9eb4db2d9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3b977385c274f4e9c0707fba3f21453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1\n",
      "Batch 50 | loss = 0.249\n",
      "Batch 100 | loss = 0.101\n",
      "Batch 150 | loss = 0.078\n",
      "Batch 200 | loss = 0.073\n",
      "Batch 250 | loss = 0.074\n",
      "Batch 300 | loss = 0.065\n",
      "Batch 350 | loss = 0.058\n",
      "Batch 400 | loss = 0.056\n",
      "Batch 450 | loss = 0.063\n",
      "Batch 500 | loss = 0.072\n",
      "Batch 550 | loss = 0.074\n",
      "Batch 600 | loss = 0.062\n",
      "Batch 650 | loss = 0.056\n",
      "Batch 700 | loss = 0.055\n",
      "Batch 750 | loss = 0.053\n",
      "Batch 800 | loss = 0.048\n",
      "Batch 850 | loss = 0.052\n",
      "Batch 900 | loss = 0.056\n",
      "Batch 950 | loss = 0.053\n",
      "Batch 1000 | loss = 0.059\n",
      "Batch 1050 | loss = 0.050\n",
      "Batch 1100 | loss = 0.061\n",
      "Batch 1150 | loss = 0.048\n",
      "Batch 1200 | loss = 0.053\n",
      "Batch 1250 | loss = 0.053\n",
      "Batch 1300 | loss = 0.055\n",
      "Batch 1350 | loss = 0.051\n",
      "Batch 1400 | loss = 0.052\n",
      "Batch 1450 | loss = 0.054\n",
      "Batch 1500 | loss = 0.054\n",
      "Batch 1550 | loss = 0.052\n",
      "Batch 1600 | loss = 0.057\n",
      "Batch 1650 | loss = 0.052\n",
      "Batch 1700 | loss = 0.045\n",
      "Batch 1750 | loss = 0.037\n",
      "Batch 1800 | loss = 0.055\n",
      "Batch 1850 | loss = 0.059\n",
      "Batch 1900 | loss = 0.054\n",
      "Batch 1950 | loss = 0.049\n",
      "Batch 2000 | loss = 0.052\n",
      "Batch 2050 | loss = 0.059\n",
      "Batch 2100 | loss = 0.044\n",
      "Batch 2150 | loss = 0.040\n",
      "Batch 2200 | loss = 0.049\n",
      "Batch 2250 | loss = 0.064\n",
      "Batch 2300 | loss = 0.058\n",
      "Batch 2350 | loss = 0.047\n",
      "Batch 2400 | loss = 0.041\n",
      "Batch 2450 | loss = 0.043\n",
      "Batch 2500 | loss = 0.046\n",
      "Batch 2550 | loss = 0.055\n",
      "Batch 2600 | loss = 0.051\n",
      "Batch 2650 | loss = 0.046\n",
      "Batch 2700 | loss = 0.052\n",
      "Batch 2750 | loss = 0.053\n",
      "Batch 2800 | loss = 0.056\n",
      "Batch 2850 | loss = 0.050\n",
      "Batch 2900 | loss = 0.048\n",
      "Batch 2950 | loss = 0.048\n",
      "Batch 3000 | loss = 0.054\n",
      "Batch 3050 | loss = 0.045\n",
      "Batch 3100 | loss = 0.055\n",
      "Batch 3150 | loss = 0.053\n",
      "Batch 3200 | loss = 0.055\n",
      "Batch 3250 | loss = 0.047\n",
      "Batch 3300 | loss = 0.043\n",
      "Batch 3350 | loss = 0.058\n",
      "Batch 3400 | loss = 0.046\n",
      "Batch 3450 | loss = 0.040\n",
      "Batch 3500 | loss = 0.049\n",
      "Batch 3550 | loss = 0.058\n",
      "Batch 3600 | loss = 0.057\n",
      "Batch 3650 | loss = 0.044\n",
      "Batch 3700 | loss = 0.055\n",
      "Batch 3750 | loss = 0.060\n",
      "Batch 3800 | loss = 0.051\n",
      "Batch 3850 | loss = 0.053\n",
      "Batch 3900 | loss = 0.036\n",
      "Batch 3950 | loss = 0.046\n",
      "Batch 4000 | loss = 0.040\n",
      "Batch 4050 | loss = 0.041\n",
      "Batch 4100 | loss = 0.049\n",
      "Batch 4150 | loss = 0.042\n",
      "Batch 4200 | loss = 0.050\n",
      "Batch 4250 | loss = 0.055\n",
      "Batch 4300 | loss = 0.043\n",
      "Batch 4350 | loss = 0.056\n",
      "Batch 4400 | loss = 0.060\n",
      "Batch 4450 | loss = 0.048\n",
      "Batch 4500 | loss = 0.047\n",
      "Batch 4550 | loss = 0.046\n",
      "Batch 4600 | loss = 0.045\n",
      "Batch 4650 | loss = 0.041\n",
      "Batch 4700 | loss = 0.051\n",
      "Batch 4750 | loss = 0.058\n",
      "Batch 4800 | loss = 0.051\n",
      "Batch 4850 | loss = 0.040\n",
      "Batch 4900 | loss = 0.046\n",
      "Batch 4950 | loss = 0.047\n",
      "Batch 5000 | loss = 0.046\n",
      "EPOCH 2\n",
      "Batch 50 | loss = 0.036\n",
      "Batch 100 | loss = 0.037\n",
      "Batch 150 | loss = 0.040\n",
      "Batch 200 | loss = 0.034\n",
      "Batch 250 | loss = 0.038\n",
      "Batch 300 | loss = 0.036\n",
      "Batch 350 | loss = 0.039\n",
      "Batch 400 | loss = 0.049\n",
      "Batch 450 | loss = 0.034\n",
      "Batch 500 | loss = 0.038\n",
      "Batch 550 | loss = 0.038\n",
      "Batch 600 | loss = 0.039\n",
      "Batch 650 | loss = 0.039\n",
      "Batch 700 | loss = 0.035\n",
      "Batch 750 | loss = 0.038\n",
      "Batch 800 | loss = 0.041\n",
      "Batch 850 | loss = 0.045\n",
      "Batch 900 | loss = 0.038\n",
      "Batch 950 | loss = 0.039\n",
      "Batch 1000 | loss = 0.038\n",
      "Batch 1050 | loss = 0.033\n",
      "Batch 1100 | loss = 0.041\n",
      "Batch 1150 | loss = 0.043\n",
      "Batch 1200 | loss = 0.038\n",
      "Batch 1250 | loss = 0.042\n",
      "Batch 1300 | loss = 0.036\n",
      "Batch 1350 | loss = 0.036\n",
      "Batch 1400 | loss = 0.036\n",
      "Batch 1450 | loss = 0.039\n",
      "Batch 1500 | loss = 0.039\n",
      "Batch 1550 | loss = 0.043\n",
      "Batch 1600 | loss = 0.035\n",
      "Batch 1650 | loss = 0.045\n",
      "Batch 1700 | loss = 0.035\n",
      "Batch 1750 | loss = 0.043\n",
      "Batch 1800 | loss = 0.034\n",
      "Batch 1850 | loss = 0.037\n",
      "Batch 1900 | loss = 0.030\n",
      "Batch 1950 | loss = 0.048\n",
      "Batch 2000 | loss = 0.035\n",
      "Batch 2050 | loss = 0.043\n",
      "Batch 2100 | loss = 0.047\n",
      "Batch 2150 | loss = 0.041\n",
      "Batch 2200 | loss = 0.038\n",
      "Batch 2250 | loss = 0.030\n",
      "Batch 2300 | loss = 0.037\n",
      "Batch 2350 | loss = 0.031\n",
      "Batch 2400 | loss = 0.037\n",
      "Batch 2450 | loss = 0.035\n",
      "Batch 2500 | loss = 0.039\n",
      "Batch 2550 | loss = 0.037\n",
      "Batch 2600 | loss = 0.037\n",
      "Batch 2650 | loss = 0.039\n",
      "Batch 2700 | loss = 0.040\n",
      "Batch 2750 | loss = 0.035\n",
      "Batch 2800 | loss = 0.039\n",
      "Batch 2850 | loss = 0.042\n",
      "Batch 2900 | loss = 0.038\n",
      "Batch 2950 | loss = 0.040\n",
      "Batch 3000 | loss = 0.039\n",
      "Batch 3050 | loss = 0.026\n",
      "Batch 3100 | loss = 0.037\n",
      "Batch 3150 | loss = 0.036\n",
      "Batch 3200 | loss = 0.027\n",
      "Batch 3250 | loss = 0.033\n",
      "Batch 3300 | loss = 0.036\n",
      "Batch 3350 | loss = 0.035\n",
      "Batch 3400 | loss = 0.030\n",
      "Batch 3450 | loss = 0.035\n",
      "Batch 3500 | loss = 0.036\n",
      "Batch 3550 | loss = 0.037\n",
      "Batch 3600 | loss = 0.038\n",
      "Batch 3650 | loss = 0.040\n",
      "Batch 3700 | loss = 0.035\n",
      "Batch 3750 | loss = 0.044\n",
      "Batch 3800 | loss = 0.032\n",
      "Batch 3850 | loss = 0.035\n",
      "Batch 3900 | loss = 0.033\n",
      "Batch 3950 | loss = 0.034\n",
      "Batch 4000 | loss = 0.036\n",
      "Batch 4050 | loss = 0.043\n",
      "Batch 4100 | loss = 0.047\n",
      "Batch 4150 | loss = 0.036\n",
      "Batch 4200 | loss = 0.043\n",
      "Batch 4250 | loss = 0.032\n",
      "Batch 4300 | loss = 0.035\n",
      "Batch 4350 | loss = 0.030\n",
      "Batch 4400 | loss = 0.038\n",
      "Batch 4450 | loss = 0.040\n",
      "Batch 4500 | loss = 0.031\n",
      "Batch 4550 | loss = 0.033\n",
      "Batch 4600 | loss = 0.029\n",
      "Batch 4650 | loss = 0.034\n",
      "Batch 4700 | loss = 0.038\n",
      "Batch 4750 | loss = 0.035\n",
      "Batch 4800 | loss = 0.027\n",
      "Batch 4850 | loss = 0.037\n",
      "Batch 4900 | loss = 0.031\n",
      "Batch 4950 | loss = 0.035\n",
      "Batch 5000 | loss = 0.034\n",
      "EPOCH 3\n",
      "Batch 50 | loss = 0.028\n",
      "Batch 100 | loss = 0.028\n",
      "Batch 150 | loss = 0.026\n",
      "Batch 200 | loss = 0.025\n",
      "Batch 250 | loss = 0.027\n",
      "Batch 300 | loss = 0.026\n",
      "Batch 350 | loss = 0.025\n",
      "Batch 400 | loss = 0.028\n",
      "Batch 450 | loss = 0.019\n",
      "Batch 500 | loss = 0.026\n",
      "Batch 550 | loss = 0.026\n",
      "Batch 600 | loss = 0.023\n",
      "Batch 650 | loss = 0.024\n",
      "Batch 700 | loss = 0.026\n",
      "Batch 750 | loss = 0.020\n",
      "Batch 800 | loss = 0.023\n",
      "Batch 850 | loss = 0.023\n",
      "Batch 900 | loss = 0.035\n",
      "Batch 950 | loss = 0.023\n",
      "Batch 1000 | loss = 0.021\n",
      "Batch 1050 | loss = 0.024\n",
      "Batch 1100 | loss = 0.029\n",
      "Batch 1150 | loss = 0.029\n",
      "Batch 1200 | loss = 0.029\n",
      "Batch 1250 | loss = 0.019\n",
      "Batch 1300 | loss = 0.026\n",
      "Batch 1350 | loss = 0.027\n",
      "Batch 1400 | loss = 0.029\n",
      "Batch 1450 | loss = 0.025\n",
      "Batch 1500 | loss = 0.028\n",
      "Batch 1550 | loss = 0.020\n",
      "Batch 1600 | loss = 0.026\n",
      "Batch 1650 | loss = 0.032\n",
      "Batch 1700 | loss = 0.029\n",
      "Batch 1750 | loss = 0.024\n",
      "Batch 1800 | loss = 0.022\n",
      "Batch 1850 | loss = 0.022\n",
      "Batch 1900 | loss = 0.023\n",
      "Batch 1950 | loss = 0.025\n",
      "Batch 2000 | loss = 0.023\n",
      "Batch 2050 | loss = 0.021\n",
      "Batch 2100 | loss = 0.024\n",
      "Batch 2150 | loss = 0.028\n",
      "Batch 2200 | loss = 0.025\n",
      "Batch 2250 | loss = 0.032\n",
      "Batch 2300 | loss = 0.024\n",
      "Batch 2350 | loss = 0.022\n",
      "Batch 2400 | loss = 0.020\n",
      "Batch 2450 | loss = 0.025\n",
      "Batch 2500 | loss = 0.027\n",
      "Batch 2550 | loss = 0.026\n",
      "Batch 2600 | loss = 0.023\n",
      "Batch 2650 | loss = 0.019\n",
      "Batch 2700 | loss = 0.024\n",
      "Batch 2750 | loss = 0.022\n",
      "Batch 2800 | loss = 0.026\n",
      "Batch 2850 | loss = 0.028\n",
      "Batch 2900 | loss = 0.023\n",
      "Batch 2950 | loss = 0.022\n",
      "Batch 3000 | loss = 0.025\n",
      "Batch 3050 | loss = 0.027\n",
      "Batch 3100 | loss = 0.025\n",
      "Batch 3150 | loss = 0.034\n",
      "Batch 3200 | loss = 0.020\n",
      "Batch 3250 | loss = 0.026\n",
      "Batch 3300 | loss = 0.028\n",
      "Batch 3350 | loss = 0.027\n",
      "Batch 3400 | loss = 0.024\n",
      "Batch 3450 | loss = 0.028\n",
      "Batch 3500 | loss = 0.021\n",
      "Batch 3550 | loss = 0.027\n",
      "Batch 3600 | loss = 0.028\n",
      "Batch 3650 | loss = 0.018\n",
      "Batch 3700 | loss = 0.027\n",
      "Batch 3750 | loss = 0.026\n",
      "Batch 3800 | loss = 0.026\n",
      "Batch 3850 | loss = 0.025\n",
      "Batch 3900 | loss = 0.031\n",
      "Batch 3950 | loss = 0.025\n",
      "Batch 4000 | loss = 0.024\n",
      "Batch 4050 | loss = 0.022\n",
      "Batch 4100 | loss = 0.027\n",
      "Batch 4150 | loss = 0.022\n",
      "Batch 4200 | loss = 0.025\n",
      "Batch 4250 | loss = 0.027\n",
      "Batch 4300 | loss = 0.022\n",
      "Batch 4350 | loss = 0.021\n",
      "Batch 4400 | loss = 0.022\n",
      "Batch 4450 | loss = 0.025\n",
      "Batch 4500 | loss = 0.024\n",
      "Batch 4550 | loss = 0.029\n",
      "Batch 4600 | loss = 0.026\n",
      "Batch 4650 | loss = 0.020\n",
      "Batch 4700 | loss = 0.036\n",
      "Batch 4750 | loss = 0.028\n",
      "Batch 4800 | loss = 0.020\n",
      "Batch 4850 | loss = 0.024\n",
      "Batch 4900 | loss = 0.023\n",
      "Batch 4950 | loss = 0.016\n",
      "Batch 5000 | loss = 0.024\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Training Loop\n",
    "\"\"\"\n",
    "training_bar = tqdm(range(N_TRAIN_STEPS))\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "tb_writer = SummaryWriter(log_dir=\"./tensorboard/runs\")\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    print(f\"EPOCH {epoch + 1}\")\n",
    "    # Phase 1 (train)\n",
    "    accumulate_loss = 0.\n",
    "    train_loss = 0.\n",
    "    model.train()\n",
    "    for idx, batch in enumerate(train_dataloader):\n",
    "        inputs = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss # compute loss\n",
    "        accumulate_loss += loss.item() # accumulate loss for visualizing later\n",
    "\n",
    "        n_batch = 50\n",
    "        if (idx + 1) % n_batch == 0:\n",
    "            train_loss = accumulate_loss / n_batch\n",
    "            print(f\"Batch {idx + 1} | loss = {train_loss:.3f}\")\n",
    "            global_step = epoch * len(train_dataloader) + (idx + 1)\n",
    "            tb_writer.add_scalar(tag=\"Train Loss\",\n",
    "                                 scalar_value=train_loss,\n",
    "                                 global_step=global_step)\n",
    "            accumulate_loss = 0.\n",
    "\n",
    "        # compute loss grads w.r.t the model's params\n",
    "        loss.backward()\n",
    "        # update params + optimizer\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        # zero out prev gradients\n",
    "        optimizer.zero_grad()\n",
    "        training_bar.update(1)\n",
    "\n",
    "    # Phase 2 (eval)\n",
    "    validation_loss = 0.\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(val_dataloader):\n",
    "            inputs = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs.loss\n",
    "            validation_loss += loss.item()\n",
    "    validation_loss = validation_loss / (idx + 1)\n",
    "    tb_writer.add_scalars(main_tag=\"\",\n",
    "                       tag_scalar_dict={\"Training\": train_loss,\n",
    "                                        \"Validation\": validation_loss},\n",
    "                       global_step=epoch+1)\n",
    "    tb_writer.flush()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
